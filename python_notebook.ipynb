{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37puETfgRzzg"
   },
   "source": [
    "# To Predict Depression Among Patients Admitted to ICU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "## Part I: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:95% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adjust notebook settings to widen the notebook\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:95% !important;}</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-qiINBQSK2g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "### Import modules/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwEPNDWySTKm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chiufengyap/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/Users/chiufengyap/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# every unique hospitalization for each patient in the database (defines HADM_ID_\n",
    "admissions = pd.read_csv('/Users/chiufengyap/OneDrive - The University of Texas Health Science Center at Houston/Research/MIMIC/mimic-iii-clinical-database-1.4/ADMISSIONS.csv')\n",
    "# Diagnosis Related Groups (DRG), which are used by the hospital for billing purposes.\n",
    "drgcodes = pd.read_csv(\"/Users/chiufengyap/OneDrive - The University of Texas Health Science Center at Houston/Research/MIMIC/mimic-iii-clinical-database-1.4/DRGCODES.csv\")\n",
    "# Deidentified notes, including nursing and physician notes, ECG reports, imaging reports, and discharge summaries.\n",
    "noteevents = pd.read_csv(\"/Users/chiufengyap/OneDrive - The University of Texas Health Science Center at Houston/Research/MIMIC/mimic-iii-clinical-database-1.4/NOTEEVENTS.csv\")\n",
    "# every unique patient in the database (defines subject_id)\n",
    "patients = pd.read_csv(\"/Users/chiufengyap/OneDrive - The University of Texas Health Science Center at Houston/Research/MIMIC/mimic-iii-clinical-database-1.4/PATIENTS.csv\")\n",
    "# the clinical service under which a patient is registered\n",
    "services = pd.read_csv(\"/Users/chiufengyap/OneDrive - The University of Texas Health Science Center at Houston/Research/MIMIC/mimic-iii-clinical-database-1.4/SERVICES.csv\")\n",
    "# Medications ordered, and not necessarily administered, for a given patient\n",
    "prescriptions = pd.read_csv(\"/Users/chiufengyap/OneDrive - The University of Texas Health Science Center at Houston/Research/MIMIC/mimic-iii-clinical-database-1.4/PRESCRIPTIONS.csv\")\n",
    "# Ground truth dataset\n",
    "phenotypes = pd.read_csv(\"/Users/chiufengyap/OneDrive - The University of Texas Health Science Center at Houston/Research/MIMIC/phenotype-annotations-for-patient-notes-in-the-mimic-iii-database-1.20.03/ACTdb102003.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all strings inside of a dataframe to lowercase\n",
    "admissions = admissions.apply(lambda x: x.astype(str).str.lower())\n",
    "drgcodes = drgcodes.apply(lambda x: x.astype(str).str.lower())\n",
    "noteevents = noteevents.apply(lambda x: x.astype(str).str.lower())\n",
    "patients = patients.apply(lambda x: x.astype(str).str.lower())\n",
    "services = services.apply(lambda x: x.astype(str).str.lower())\n",
    "prescriptions = prescriptions.apply(lambda x: x.astype(str).str.lower())\n",
    "phenotypes = phenotypes.apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "# lowercase columns in all dataframes\n",
    "admissions.columns = admissions.columns.str.lower()\n",
    "drgcodes.columns = drgcodes.columns.str.lower()\n",
    "noteevents.columns = noteevents.columns.str.lower()\n",
    "patients.columns = patients.columns.str.lower()\n",
    "services.columns = services.columns.str.lower()\n",
    "prescriptions.columns = prescriptions.columns.str.lower()\n",
    "phenotypes.columns = phenotypes.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decrease the datasets by subsetting the records which ID is in phenotypes dataset\n",
    "admissions_reduced = admissions[admissions['subject_id'].isin(phenotypes['subject_id'])]\n",
    "drgcodes_reduced = drgcodes[drgcodes['subject_id'].isin(phenotypes['subject_id'])]\n",
    "noteevents_reduced = noteevents[noteevents['subject_id'].isin(phenotypes['subject_id'])]\n",
    "patients_reduced = patients[patients['subject_id'].isin(phenotypes['subject_id'])]\n",
    "services_reduced = services[services['subject_id'].isin(phenotypes['subject_id'])]\n",
    "prescriptions_reduced = prescriptions[prescriptions['subject_id'].isin(phenotypes['subject_id'])]\n",
    "\n",
    "\n",
    "admissions_reduced = admissions_reduced.reset_index(drop=True)\n",
    "drgcodes_reduced = drgcodes_reduced.reset_index(drop=True)\n",
    "noteevents_reduced = noteevents_reduced.reset_index(drop=True)\n",
    "patients_reduced = patients_reduced.reset_index(drop=True)\n",
    "services_reduced = services_reduced.reset_index(drop=True)\n",
    "prescriptions_reduced = prescriptions_reduced.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get unique values\n",
    "def unique(list1):\n",
    "    x = np.array(list1)\n",
    "    print(np.unique(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dataset: phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the interested outcome feature\n",
    "phenotypes_reduced = phenotypes[['hadm_id','subject_id','depression']]\n",
    "# Drop duplicated records by subject_id and hadm_id\n",
    "phenotypes_reduced = phenotypes_reduced.drop_duplicates(subset=['subject_id','hadm_id'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of phenotypes_reduced\n",
    "phenotypes_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100103</td>\n",
       "      <td>3365</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100137</td>\n",
       "      <td>27290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100473</td>\n",
       "      <td>5525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100485</td>\n",
       "      <td>41515</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100548</td>\n",
       "      <td>2265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hadm_id subject_id depression\n",
       "0  100103       3365          0\n",
       "1  100137      27290          0\n",
       "2  100473       5525          0\n",
       "3  100485      41515          1\n",
       "4  100548       2265          0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phenotypes_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dataset: admissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding diagnosis feature from admission: \n",
    "\n",
    "15,693 distinct diagnoses for 58,976 admissions. The diagnoses can be very informative (e.g. chronic kidney failure) or quite vague (e.g. weakness). Final diagnoses for a patientâ€™s hospital stay are coded on discharge and can be found in the DIAGNOSES_ICD table. While this field can provide information about the status of a patient on hospital admission, it is not recommended to use it to stratify patients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1944, 19)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of admissions_reduced\n",
    "admissions_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the dates and times from the database are deidentified -- create new features to get the time difference\n",
    "admissions_reduced['edouttime'] = pd.to_datetime(admissions_reduced['edouttime'])\n",
    "admissions_reduced['edregtime'] = pd.to_datetime(admissions_reduced['edregtime'])\n",
    "admissions_reduced['length_ed'] = (admissions_reduced['edouttime'] - admissions_reduced['edregtime']).dt.days\n",
    "admissions_reduced['dischtime'] = pd.to_datetime(admissions_reduced['dischtime'])\n",
    "admissions_reduced['admittime'] = pd.to_datetime(admissions_reduced['admittime'])\n",
    "admissions_reduced['length_admit'] = (admissions_reduced['dischtime'] - admissions_reduced['admittime']).dt.days\n",
    "# Drop time-related features used to create new features\n",
    "admissions_reduced = admissions_reduced.drop(['edregtime', 'edouttime', 'dischtime', 'admittime'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregate, dummy, and new variables for admission df to create one row per id\n",
    "just_dummies = pd.get_dummies(admissions_reduced['admission_type'], prefix='admission_type')\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for admission location\n",
    "just_dummies = pd.get_dummies(admissions_reduced['admission_location'], prefix='admission_loc')\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for discharge location\n",
    "just_dummies = pd.get_dummies(admissions_reduced['discharge_location'], prefix='discharge_loc')\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for insurance\n",
    "just_dummies = pd.get_dummies(admissions_reduced['insurance'], prefix='insurance')\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for religions\n",
    "just_dummies = pd.get_dummies(admissions_reduced['religion'], prefix='religion')\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for language\n",
    "just_dummies = pd.get_dummies(admissions_reduced['language'], prefix='language')\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for marital_status\n",
    "just_dummies = pd.get_dummies(admissions_reduced['marital_status'], prefix='marital_status')\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for ethnicity\n",
    "just_dummies = pd.get_dummies(admissions_reduced['ethnicity'], prefix='ethnicity')\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features used to dummy variables\n",
    "admissions_reduced = admissions_reduced.drop(['row_id', 'deathtime', 'diagnosis', 'religion', 'language','marital_status', 'ethnicity', 'insurance', 'admission_location', 'discharge_location', 'admission_type'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "      <th>has_chartevents_data</th>\n",
       "      <th>length_ed</th>\n",
       "      <th>length_admit</th>\n",
       "      <th>admission_type_elective</th>\n",
       "      <th>admission_type_emergency</th>\n",
       "      <th>admission_type_urgent</th>\n",
       "      <th>admission_loc_clinic referral/premature</th>\n",
       "      <th>admission_loc_emergency room admit</th>\n",
       "      <th>admission_loc_phys referral/normal deli</th>\n",
       "      <th>admission_loc_transfer from hosp/extram</th>\n",
       "      <th>admission_loc_transfer from other healt</th>\n",
       "      <th>admission_loc_transfer from skilled nur</th>\n",
       "      <th>discharge_loc_dead/expired</th>\n",
       "      <th>discharge_loc_disc-tran cancer/chldrn h</th>\n",
       "      <th>discharge_loc_disch-tran to psych hosp</th>\n",
       "      <th>discharge_loc_home</th>\n",
       "      <th>discharge_loc_home health care</th>\n",
       "      <th>discharge_loc_home with home iv providr</th>\n",
       "      <th>discharge_loc_hospice-home</th>\n",
       "      <th>discharge_loc_hospice-medical facility</th>\n",
       "      <th>discharge_loc_icf</th>\n",
       "      <th>discharge_loc_left against medical advi</th>\n",
       "      <th>discharge_loc_long term care hospital</th>\n",
       "      <th>discharge_loc_other facility</th>\n",
       "      <th>discharge_loc_rehab/distinct part hosp</th>\n",
       "      <th>discharge_loc_short term hospital</th>\n",
       "      <th>discharge_loc_snf</th>\n",
       "      <th>insurance_government</th>\n",
       "      <th>insurance_medicaid</th>\n",
       "      <th>insurance_medicare</th>\n",
       "      <th>insurance_private</th>\n",
       "      <th>insurance_self pay</th>\n",
       "      <th>religion_baptist</th>\n",
       "      <th>religion_buddhist</th>\n",
       "      <th>religion_catholic</th>\n",
       "      <th>religion_christian scientist</th>\n",
       "      <th>religion_episcopalian</th>\n",
       "      <th>religion_greek orthodox</th>\n",
       "      <th>religion_hebrew</th>\n",
       "      <th>religion_jehovah's witness</th>\n",
       "      <th>religion_jewish</th>\n",
       "      <th>religion_muslim</th>\n",
       "      <th>religion_nan</th>\n",
       "      <th>religion_not specified</th>\n",
       "      <th>religion_other</th>\n",
       "      <th>religion_protestant quaker</th>\n",
       "      <th>religion_romanian east. orth</th>\n",
       "      <th>religion_unitarian-universalist</th>\n",
       "      <th>religion_unobtainable</th>\n",
       "      <th>language_*chi</th>\n",
       "      <th>language_*hun</th>\n",
       "      <th>language_*man</th>\n",
       "      <th>language_arab</th>\n",
       "      <th>language_camb</th>\n",
       "      <th>language_cant</th>\n",
       "      <th>language_cape</th>\n",
       "      <th>language_engl</th>\n",
       "      <th>language_fren</th>\n",
       "      <th>language_gree</th>\n",
       "      <th>language_hait</th>\n",
       "      <th>language_ital</th>\n",
       "      <th>language_nan</th>\n",
       "      <th>language_pers</th>\n",
       "      <th>language_port</th>\n",
       "      <th>language_ptun</th>\n",
       "      <th>language_russ</th>\n",
       "      <th>language_span</th>\n",
       "      <th>language_urdu</th>\n",
       "      <th>marital_status_divorced</th>\n",
       "      <th>marital_status_life partner</th>\n",
       "      <th>marital_status_married</th>\n",
       "      <th>marital_status_nan</th>\n",
       "      <th>marital_status_separated</th>\n",
       "      <th>marital_status_single</th>\n",
       "      <th>marital_status_unknown (default)</th>\n",
       "      <th>marital_status_widowed</th>\n",
       "      <th>ethnicity_asian</th>\n",
       "      <th>ethnicity_asian - asian indian</th>\n",
       "      <th>ethnicity_asian - chinese</th>\n",
       "      <th>ethnicity_black/african</th>\n",
       "      <th>ethnicity_black/african american</th>\n",
       "      <th>ethnicity_black/cape verdean</th>\n",
       "      <th>ethnicity_black/haitian</th>\n",
       "      <th>ethnicity_hispanic or latino</th>\n",
       "      <th>ethnicity_hispanic/latino - guatemalan</th>\n",
       "      <th>ethnicity_hispanic/latino - puerto rican</th>\n",
       "      <th>ethnicity_other</th>\n",
       "      <th>ethnicity_patient declined to answer</th>\n",
       "      <th>ethnicity_portuguese</th>\n",
       "      <th>ethnicity_unable to obtain</th>\n",
       "      <th>ethnicity_unknown/not specified</th>\n",
       "      <th>ethnicity_white</th>\n",
       "      <th>ethnicity_white - brazilian</th>\n",
       "      <th>ethnicity_white - eastern european</th>\n",
       "      <th>ethnicity_white - russian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>368</td>\n",
       "      <td>105889</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>368</td>\n",
       "      <td>138061</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject_id hadm_id hospital_expire_flag has_chartevents_data  length_ed  \\\n",
       "0        368  105889                    0                    1        0.0   \n",
       "1        368  138061                    0                    1        1.0   \n",
       "\n",
       "   length_admit  admission_type_elective  admission_type_emergency  \\\n",
       "0             4                        0                         1   \n",
       "1             5                        0                         1   \n",
       "\n",
       "   admission_type_urgent  admission_loc_clinic referral/premature  \\\n",
       "0                      0                                        0   \n",
       "1                      0                                        0   \n",
       "\n",
       "   admission_loc_emergency room admit  \\\n",
       "0                                   1   \n",
       "1                                   1   \n",
       "\n",
       "   admission_loc_phys referral/normal deli  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "\n",
       "   admission_loc_transfer from hosp/extram  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "\n",
       "   admission_loc_transfer from other healt  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "\n",
       "   admission_loc_transfer from skilled nur  discharge_loc_dead/expired  \\\n",
       "0                                        0                           0   \n",
       "1                                        0                           0   \n",
       "\n",
       "   discharge_loc_disc-tran cancer/chldrn h  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "\n",
       "   discharge_loc_disch-tran to psych hosp  discharge_loc_home  \\\n",
       "0                                       0                   0   \n",
       "1                                       0                   0   \n",
       "\n",
       "   discharge_loc_home health care  discharge_loc_home with home iv providr  \\\n",
       "0                               0                                        0   \n",
       "1                               0                                        0   \n",
       "\n",
       "   discharge_loc_hospice-home  discharge_loc_hospice-medical facility  \\\n",
       "0                           0                                       0   \n",
       "1                           0                                       0   \n",
       "\n",
       "   discharge_loc_icf  discharge_loc_left against medical advi  \\\n",
       "0                  0                                        0   \n",
       "1                  0                                        0   \n",
       "\n",
       "   discharge_loc_long term care hospital  discharge_loc_other facility  \\\n",
       "0                                      0                             0   \n",
       "1                                      0                             0   \n",
       "\n",
       "   discharge_loc_rehab/distinct part hosp  discharge_loc_short term hospital  \\\n",
       "0                                       0                                  0   \n",
       "1                                       0                                  0   \n",
       "\n",
       "   discharge_loc_snf  insurance_government  insurance_medicaid  \\\n",
       "0                  1                     0                   0   \n",
       "1                  1                     0                   0   \n",
       "\n",
       "   insurance_medicare  insurance_private  insurance_self pay  \\\n",
       "0                   1                  0                   0   \n",
       "1                   1                  0                   0   \n",
       "\n",
       "   religion_baptist  religion_buddhist  religion_catholic  \\\n",
       "0                 0                  0                  0   \n",
       "1                 0                  0                  0   \n",
       "\n",
       "   religion_christian scientist  religion_episcopalian  \\\n",
       "0                             0                      0   \n",
       "1                             0                      0   \n",
       "\n",
       "   religion_greek orthodox  religion_hebrew  religion_jehovah's witness  \\\n",
       "0                        0                0                           0   \n",
       "1                        0                0                           0   \n",
       "\n",
       "   religion_jewish  religion_muslim  religion_nan  religion_not specified  \\\n",
       "0                0                0             0                       1   \n",
       "1                0                0             0                       1   \n",
       "\n",
       "   religion_other  religion_protestant quaker  religion_romanian east. orth  \\\n",
       "0               0                           0                             0   \n",
       "1               0                           0                             0   \n",
       "\n",
       "   religion_unitarian-universalist  religion_unobtainable  language_*chi  \\\n",
       "0                                0                      0              0   \n",
       "1                                0                      0              0   \n",
       "\n",
       "   language_*hun  language_*man  language_arab  language_camb  language_cant  \\\n",
       "0              0              0              0              0              0   \n",
       "1              0              0              0              0              0   \n",
       "\n",
       "   language_cape  language_engl  language_fren  language_gree  language_hait  \\\n",
       "0              0              0              0              0              0   \n",
       "1              0              0              0              0              0   \n",
       "\n",
       "   language_ital  language_nan  language_pers  language_port  language_ptun  \\\n",
       "0              0             1              0              0              0   \n",
       "1              0             1              0              0              0   \n",
       "\n",
       "   language_russ  language_span  language_urdu  marital_status_divorced  \\\n",
       "0              0              0              0                        0   \n",
       "1              0              0              0                        0   \n",
       "\n",
       "   marital_status_life partner  marital_status_married  marital_status_nan  \\\n",
       "0                            0                       0                   0   \n",
       "1                            0                       0                   0   \n",
       "\n",
       "   marital_status_separated  marital_status_single  \\\n",
       "0                         0                      0   \n",
       "1                         0                      0   \n",
       "\n",
       "   marital_status_unknown (default)  marital_status_widowed  ethnicity_asian  \\\n",
       "0                                 0                       1                0   \n",
       "1                                 0                       1                0   \n",
       "\n",
       "   ethnicity_asian - asian indian  ethnicity_asian - chinese  \\\n",
       "0                               0                          0   \n",
       "1                               0                          0   \n",
       "\n",
       "   ethnicity_black/african  ethnicity_black/african american  \\\n",
       "0                        0                                 0   \n",
       "1                        0                                 0   \n",
       "\n",
       "   ethnicity_black/cape verdean  ethnicity_black/haitian  \\\n",
       "0                             0                        0   \n",
       "1                             0                        0   \n",
       "\n",
       "   ethnicity_hispanic or latino  ethnicity_hispanic/latino - guatemalan  \\\n",
       "0                             0                                       0   \n",
       "1                             0                                       0   \n",
       "\n",
       "   ethnicity_hispanic/latino - puerto rican  ethnicity_other  \\\n",
       "0                                         0                0   \n",
       "1                                         0                0   \n",
       "\n",
       "   ethnicity_patient declined to answer  ethnicity_portuguese  \\\n",
       "0                                     0                     0   \n",
       "1                                     0                     0   \n",
       "\n",
       "   ethnicity_unable to obtain  ethnicity_unknown/not specified  \\\n",
       "0                           0                                0   \n",
       "1                           0                                0   \n",
       "\n",
       "   ethnicity_white  ethnicity_white - brazilian  \\\n",
       "0                1                            0   \n",
       "1                1                            0   \n",
       "\n",
       "   ethnicity_white - eastern european  ethnicity_white - russian  \n",
       "0                                   0                          0  \n",
       "1                                   0                          0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions_reduced.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions_reduced[\"hospital_expire_flag\"] = admissions_reduced.hospital_expire_flag.astype(float)\n",
    "admissions_reduced[\"has_chartevents_data\"] = admissions_reduced.has_chartevents_data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1944, 98)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of cleaned admissions_reduced dataset\n",
    "admissions_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dataset: patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of patients_reduced dataset\n",
    "patients_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numerical code for string variables in the gender feature\n",
    "patients_reduced.gender[patients_reduced.gender == 'm'] = 1\n",
    "patients_reduced.gender[patients_reduced.gender == 'f'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing PIH features that had been deidentified\n",
    "patients_reduced = patients_reduced.drop(['row_id', 'dob', 'dod', 'dod_hosp', 'dod_ssn'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main[\"expire_flag\"] = main.expire_flag.astype(float)\n",
    "main['gender'] = main.gender.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final features left for patients_reduced dataset\n",
    "patients_reduced.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning dataset:  drgcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drgcodes_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drgcodes_reduced.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drgcodes_reduced.drg_severity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drgcodes_reduced.drg_mortality.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for drg_code\n",
    "just_dummies = pd.get_dummies(drgcodes_reduced['drg_code'], prefix='drg_code')\n",
    "drgcodes_reduced = pd.concat([drgcodes_reduced, just_dummies], axis=1)\n",
    "\n",
    "# Create dummy variables for drg_code\n",
    "just_dummies = pd.get_dummies(drgcodes_reduced['drg_type'], prefix='drg_type')\n",
    "drgcodes_reduced = pd.concat([drgcodes_reduced, just_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform object to numerical features\n",
    "drgcodes_reduced['drg_mortality'] = pd.to_numeric(drgcodes_reduced.drg_mortality, errors='coerce').fillna(0, downcast='infer').astype('Int32')\n",
    "drgcodes_reduced['drg_severity'] = pd.to_numeric(drgcodes_reduced.drg_severity, errors='coerce').fillna(0, downcast='infer').astype('Int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to have one record for each unique combination of subject_id and hadm_id, mean of the drg_mortality and drg_severity\n",
    "# are calculated\n",
    "drgcodes_reduced['avg_drg_mortality'] = drgcodes_reduced.groupby(['subject_id', 'hadm_id']).drg_mortality.transform('mean')\n",
    "drgcodes_reduced['avg_drg_severity'] = drgcodes_reduced.groupby(['subject_id', 'hadm_id']).drg_severity.transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drgcodes_reduced['avg_drg_mortality'] = drgcodes_reduced.avg_drg_mortality.astype(float)\n",
    "drgcodes_reduced['avg_drg_severity'] = drgcodes_reduced.avg_drg_severity.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates by comparing subject_id and hadm_id\n",
    "drgcodes_reduced = drgcodes_reduced.drop_duplicates(subset=['subject_id','hadm_id'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drgcodes_reduced = drgcodes_reduced.drop(['row_id', 'description', 'drg_code', 'drg_type', 'drg_severity', 'drg_mortality' ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final size of the drgcodes_reduced\n",
    "drgcodes_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotypes_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main = pd.merge(admissions_reduced, phenotypes_reduced,\n",
    "                how ='right',\n",
    "                on = ['subject_id', 'hadm_id'])\n",
    "\n",
    "main = pd.merge(main, patients_reduced,\n",
    "                how ='left',\n",
    "                on = ['subject_id'])\n",
    "\n",
    "main = pd.merge(main, drgcodes_reduced,\n",
    "                how ='left',\n",
    "                on = ['subject_id', 'hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missingness of the final merged datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main.isnull().mean() # length_ed had ~30% missingness\n",
    "main['length_ed'] = main['length_ed'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructuring -- Move the outcome variable to be the last column in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(main.columns.values)\n",
    "cols.pop(cols.index('depression'))\n",
    "main = main[cols+['depression']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main[\"hospital_expire_flag\"] = main.hospital_expire_flag.astype(float)\n",
    "main[\"has_chartevents_data\"] = main.has_chartevents_data.astype(float)\n",
    "main[\"expire_flag\"] = main.expire_flag.astype(float)\n",
    "main['gender'] = main.gender.astype(int)\n",
    "main['avg_drg_mortality'] = main.avg_drg_mortality.astype(float)\n",
    "main['avg_drg_severity'] = main.avg_drg_severity.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set X as features and y as the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = main.drop(['subject_id', 'hadm_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = main.iloc[:, :-1].values\n",
    "y = main.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhSpdQWeSsFh"
   },
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hwuVddlSwVi"
   },
   "outputs": [],
   "source": [
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "# X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXh8oVSITIc6"
   },
   "source": [
    "### Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XgHCShVyTOYY"
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXgA6CzlqbCl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is essential for machine learning algorithms that calculate distances between data. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\n",
    "\n",
    "* Normalization is recommended when you have a normally distributed observations.\n",
    "* Standardization works all the time. (recommended)\n",
    "* We need to perform Feature Scaling when we are dealing with Gradient Descent Based algorithms (Linear and Logistic Regression, Neural Network) and Distance-based algorithms (KNN, K-means, SVM) as these are very sensitive to the range of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) # for Euclidean distance\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption: observations are linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maaping to a higher dimensional space can be highly compute-intensive.\n",
    "\n",
    "Types of Kernal Functions:\n",
    "* Gaussian RBF Kernel\n",
    "* Sigmoid Kernel\n",
    "* Polynomial Kernel\n",
    "* and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages: This algorithm requires a small amount of training data to estimate the necessary parameters. Naive Bayes classifiers are extremely fast compared to more sophisticated methods.\n",
    "\n",
    "Disadvantages: Naive Bayes is is known to be a bad estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages: Decision Tree is simple to understand and visualise, requires little data preparation, and can handle both numerical and categorical data.\n",
    "\n",
    "Disadvantages: Decision tree can create complex trees that do not generalise well, and decision trees can be unstable because small variations in the data might result in a completely different tree being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Learning : using different machine algorithms \n",
    "\n",
    "The algorithm does not work well for datasets having a lot of outliers, something which needs addressing prior to the model building.\n",
    "\n",
    "Advantages: Reduction in over-fitting and random forest classifier is more accurate than decision trees in most cases.\n",
    "\n",
    "Disadvantages: Slow real time prediction, difficult to implement, and complex algorithm.\n",
    "\n",
    "Build on top of Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components = 1)\n",
    "X_train = lda.fit_transform(X_train, y_train)\n",
    "X_test = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Grid Search to find the best model and the best parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbor (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbor (KNN) algorithm predicts based on the specified number (k) of the nearest neighboring data points. Here, the pre-processing of the data is significant as it impacts the distance measurements directly. Unlike others, the model does not have a mathematical formula, neither any descriptive ability.\n",
    "\n",
    "It is a simple, fairly accurate model preferable mostly for smaller datasets, owing to huge computations involved on the continuous predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Choose the number of K of neighbors\n",
    "\n",
    "Step 2: Take the K nearest neighbors of the new data point, according to the Euclidean distance\n",
    "\n",
    "Step 3: Among these K neighbors, count the number of data points in each category\n",
    "\n",
    "Step 4: Assign the new data point ot the category where you counted the most neighbors\n",
    "\n",
    "Then, the model will be ready. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) # for Euclidean distance\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption: observations are linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maaping to a higher dimensional space can be highly compute-intensive.\n",
    "\n",
    "Types of Kernal Functions:\n",
    "* Gaussian RBF Kernel\n",
    "* Sigmoid Kernel\n",
    "* Polynomial Kernel\n",
    "* and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages: This algorithm requires a small amount of training data to estimate the necessary parameters. Naive Bayes classifiers are extremely fast compared to more sophisticated methods.\n",
    "\n",
    "Disadvantages: Naive Bayes is is known to be a bad estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages: Decision Tree is simple to understand and visualise, requires little data preparation, and can handle both numerical and categorical data.\n",
    "\n",
    "Disadvantages: Decision tree can create complex trees that do not generalise well, and decision trees can be unstable because small variations in the data might result in a completely different tree being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Learning : using different machine algorithms \n",
    "\n",
    "The algorithm does not work well for datasets having a lot of outliers, something which needs addressing prior to the model building.\n",
    "\n",
    "Advantages: Reduction in over-fitting and random forest classifier is more accurate than decision trees in most cases.\n",
    "\n",
    "Disadvantages: Slow real time prediction, difficult to implement, and complex algorithm.\n",
    "\n",
    "Build on top of Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components = 2, kernel = 'rbf')\n",
    "X_train = kpca.fit_transform(X_train)\n",
    "X_test = kpca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing_tools.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
