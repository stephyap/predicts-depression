{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37puETfgRzzg"
   },
   "source": [
    "## To Predict Chronic Pain Among Patients Admitted to ICU \n",
    "\n",
    "Chronic Pain - Any etiology of chronic pain (including fibromyalgia) requiring long-term opiod/narcotic medication to control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "### Part I: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-qiINBQSK2g"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:85% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Adjust notebook settings to widen the notebook\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:85% !important;}</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "### Import modules/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwEPNDWySTKm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chiufengyap/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/Users/chiufengyap/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# every unique hospitalization for each patient in the database (defines HADM_ID_\n",
    "admissions = pd.read_csv('data/ADMISSIONS.csv')\n",
    "# every unique patient in the database (defines subject_id)\n",
    "patients = pd.read_csv(\"data/PATIENTS.csv\")\n",
    "# the clinical service under which a patient is registered\n",
    "services = pd.read_csv(\"data/SERVICES.csv\")\n",
    "# Diagnosis Related Groups (DRG), which are used by the hospital for billing purposes.\n",
    "drgcodes = pd.read_csv(\"data/DRGCODES.csv\")\n",
    "# Deidentified notes, including nursing and physician notes, ECG reports, imaging reports, and discharge summaries.\n",
    "noteevents = pd.read_csv(\"data/NOTEEVENTS.csv\")\n",
    "# Medications ordered, and not necessarily administered, for a given patient\n",
    "prescriptions = pd.read_csv(\"data/PRESCRIPTIONS.csv\")\n",
    "# Ground truth dataset\n",
    "phenotypes = pd.read_csv(\"data/GROUND_TRUTH.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all strings inside of a dataframe to lowercase\n",
    "admissions = admissions.apply(lambda x: x.astype(str).str.lower())\n",
    "drgcodes = drgcodes.apply(lambda x: x.astype(str).str.lower())\n",
    "noteevents = noteevents.apply(lambda x: x.astype(str).str.lower())\n",
    "patients = patients.apply(lambda x: x.astype(str).str.lower())\n",
    "services = services.apply(lambda x: x.astype(str).str.lower())\n",
    "prescriptions = prescriptions.apply(lambda x: x.astype(str).str.lower())\n",
    "phenotypes = phenotypes.apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "# lowercase columns in all dataframes\n",
    "admissions.columns = admissions.columns.str.lower()\n",
    "drgcodes.columns = drgcodes.columns.str.lower()\n",
    "noteevents.columns = noteevents.columns.str.lower()\n",
    "patients.columns = patients.columns.str.lower()\n",
    "services.columns = services.columns.str.lower()\n",
    "prescriptions.columns = prescriptions.columns.str.lower()\n",
    "phenotypes.columns = phenotypes.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decrease the datasets by subsetting the records which ID is in phenotypes dataset\n",
    "admissions_reduced = admissions[admissions['subject_id'].isin(phenotypes['subject_id'])]\n",
    "drgcodes_reduced = drgcodes[drgcodes['subject_id'].isin(phenotypes['subject_id'])]\n",
    "noteevents_reduced = noteevents[noteevents['subject_id'].isin(phenotypes['subject_id'])]\n",
    "patients_reduced = patients[patients['subject_id'].isin(phenotypes['subject_id'])]\n",
    "services_reduced = services[services['subject_id'].isin(phenotypes['subject_id'])]\n",
    "prescriptions_reduced = prescriptions[prescriptions['subject_id'].isin(phenotypes['subject_id'])]\n",
    "\n",
    "admissions_reduced = admissions_reduced.reset_index(drop=True)\n",
    "drgcodes_reduced = drgcodes_reduced.reset_index(drop=True)\n",
    "noteevents_reduced = noteevents_reduced.reset_index(drop=True)\n",
    "patients_reduced = patients_reduced.reset_index(drop=True)\n",
    "services_reduced = services_reduced.reset_index(drop=True)\n",
    "prescriptions_reduced = prescriptions_reduced.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes Included\n",
    "\n",
    "Link: https://mimic.mit.edu/docs/iii/tables/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_id', 'subject_id', 'hadm_id', 'admittime', 'dischtime',\n",
       "       'deathtime', 'admission_type', 'admission_location',\n",
       "       'discharge_location', 'insurance', 'language', 'religion',\n",
       "       'marital_status', 'ethnicity', 'edregtime', 'edouttime', 'diagnosis',\n",
       "       'hospital_expire_flag', 'has_chartevents_data'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUBJECT_ID, HADM_ID**\n",
    "Each row of this table contains a unique HADM_ID, which represents a single patient’s admission to the hospital. HADM_ID ranges from 1000000 - 1999999. It is possible for this table to have duplicate SUBJECT_ID, indicating that a single patient had multiple admissions to the hospital. The ADMISSIONS table can be linked to the PATIENTS table using SUBJECT_ID.\n",
    "\n",
    "**ADMITTIME, DISCHTIME, DEATHTIME**\n",
    "ADMITTIME provides the date and time the patient was admitted to the hospital, while DISCHTIME provides the date and time the patient was discharged from the hospital. If applicable, DEATHTIME provides the time of in-hospital death for the patient. Note that DEATHTIME is only present if the patient died in-hospital, and is almost always the same as the patient’s DISCHTIME. However, there can be some discrepancies due to typographical errors.\n",
    "\n",
    "**ADMISSION_TYPE**\n",
    "ADMISSION_TYPE describes the type of the admission: ‘ELECTIVE’, ‘URGENT’, ‘NEWBORN’ or ‘EMERGENCY’. Emergency/urgent indicate unplanned medical care, and are often collapsed into a single category in studies. Elective indicates a previously planned hospital admission. Newborn indicates that the HADM_ID pertains to the patient’s birth.\n",
    "\n",
    "**ADMISSION_LOCATION**\n",
    "ADMISSION_LOCATION provides information about the previous location of the patient prior to arriving at the hospital. There are 9 possible values:\n",
    "\n",
    "* EMERGENCY ROOM ADMIT\n",
    "* TRANSFER FROM HOSP/EXTRAM\n",
    "* TRANSFER FROM OTHER HEALT\n",
    "* CLINIC REFERRAL/PREMATURE\n",
    "* ** INFO NOT AVAILABLE **\n",
    "* TRANSFER FROM SKILLED NUR\n",
    "* TRSF WITHIN THIS FACILITY\n",
    "* HMO REFERRAL/SICK\n",
    "* PHYS REFERRAL/NORMAL DELI\n",
    "\n",
    "The truncated text occurs in the raw data.\n",
    "\n",
    "**INSURANCE, LANGUAGE, RELIGION, MARITAL_STATUS, ETHNICITY**\n",
    "The INSURANCE, LANGUAGE, RELIGION, MARITAL_STATUS, ETHNICITY columns describe patient demographics. These columns occur in the ADMISSIONS table as they are originally sourced from the admission, discharge, and transfers (ADT) data from the hospital database. The values occasionally change between hospital admissions (HADM_ID) for a single patient (SUBJECT_ID). This is reasonable for some fields (e.g. MARITAL_STATUS, RELIGION), but less reasonable for others (e.g. ETHNICITY).\n",
    "\n",
    "**EDREGTIME, EDOUTTIME**\n",
    "Time that the patient was registered and discharged from the emergency department.\n",
    "\n",
    "**DIAGNOSIS**\n",
    "The DIAGNOSIS column provides a preliminary, free text diagnosis for the patient on hospital admission. The diagnosis is usually assigned by the admitting clinician and does not use a systematic ontology. As of MIMIC-III v1.0 there were 15,693 distinct diagnoses for 58,976 admissions. The diagnoses can be very informative (e.g. chronic kidney failure) or quite vague (e.g. weakness). Final diagnoses for a patient’s hospital stay are coded on discharge and can be found in the DIAGNOSES_ICD table. While this field can provide information about the status of a patient on hospital admission, it is not recommended to use it to stratify patients.\n",
    "\n",
    "**HOSPITAL_EXPIRE_FLAG**\n",
    "This indicates whether the patient died within the given hospitalization. 1 indicates death in the hospital, and 0 indicates survival to hospital discharge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_id', 'subject_id', 'hadm_id', 'drg_type', 'drg_code',\n",
       "       'description', 'drg_severity', 'drg_mortality'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drgcodes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUBJECT_ID, HADM_ID**\n",
    "Identifiers which specify the patient: SUBJECT_ID is unique to a patient and HADM_ID is unique to a patient hospital stay.\n",
    "\n",
    "**DRG_TYPE**\n",
    "DRG_TYPE provides the type of DRG code in the entry. There are two types of DRG codes in the database which have overlapping ranges but distinct definitions for the codes. The three types of DRG codes in the MIMIC-III database are ‘HCFA’ (Health Care Financing Administration), ‘MS’ (Medicare), and ‘APR’ (All Payers Registry).\n",
    "\n",
    "**DRG_CODE**\n",
    "DRG_CODE contains a code which represents the diagnosis billed for by the hospital.\n",
    "\n",
    "**DESCRIPTION**\n",
    "DESCRIPTION provides a human understandable summary of the meaning of the given DRG code. The description field frequently has acronyms which represent comorbidity levels (comorbid conditions or “CC”). The following table provides a definition for some of these acronyms:\n",
    "\n",
    "Acronym\tDescription\n",
    "* w CC/MCC\twith CC or Major CC\n",
    "* w MCC\twith Major CC\n",
    "* w CC\twith CC and without Major CC\n",
    "* w NonCC\twith NonCC and without CC or Major CC\n",
    "* w/o MCC\twith CC or Non CC and without Major CC\n",
    "* w/o CC/MCC\twith nonCC and without CC or Major CC\n",
    "\n",
    "Note that there are three levels of comorbidities: none, with comorbid conditions, and with major comorbid conditions. These acronyms are primarily used in HCFA/MS DRG codes.\n",
    "\n",
    "**DRG_SEVERITY, DRG_MORTALITY, DRG_SEVERITY and DRG_MORTALITY**\n",
    "provide additional granularity to DRG codes in the ‘APR’ DRG type. Severity and mortality allow for higher billing costs when a diagnosis is more severe, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_id', 'subject_id', 'hadm_id', 'chartdate', 'charttime',\n",
       "       'storetime', 'category', 'description', 'cgid', 'iserror', 'text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noteevents.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUBJECT_ID, HADM_ID**\n",
    "Identifiers which specify the patient: SUBJECT_ID is unique to a patient and HADM_ID is unique to a patient hospital stay.\n",
    "\n",
    "**CHARTDATE, CHARTTIME, STORETIME**\n",
    "CHARTDATE records the date at which the note was charted. CHARTDATE will always have a time value of 00:00:00.\n",
    "\n",
    "CHARTTIME records the date and time at which the note was charted. If both CHARTDATE and CHARTTIME exist, then the date portions will be identical. All records have a CHARTDATE. A subset are missing CHARTTIME. More specifically, notes with a CATEGORY value of ‘Discharge Summary’, ‘ECG’, and ‘Echo’ never have a CHARTTIME, only CHARTDATE. Other categories almost always have both CHARTTIME and CHARTDATE, but there is a small amount of missing data for CHARTTIME (usually less than 0.5% of the total number of notes for that category).\n",
    "\n",
    "STORETIME records the date and time at which a note was saved into the system. Notes with a CATEGORY value of ‘Discharge Summary’, ‘ECG’, ‘Radiology’, and ‘Echo’ never have a STORETIME. All other notes have a STORETIME.\n",
    "\n",
    "**CATEGORY, DESCRIPTION**\n",
    "CATEGORY and DESCRIPTION define the type of note recorded. For example, a CATEGORY of ‘Discharge summary’ indicates that the note is a discharge summary, and the DESCRIPTION of ‘Report’ indicates a full report while a DESCRIPTION of ‘Addendum’ indicates an addendum (additional text to be added to the previous report).\n",
    "\n",
    "**CGID**\n",
    "CGID is the identifier for the caregiver who input the note.\n",
    "\n",
    "**ISERROR**\n",
    "A ‘1’ in the ISERROR column indicates that a physician has identified this note as an error.\n",
    "\n",
    "**TEXT**\n",
    "TEXT contains the note text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_id', 'subject_id', 'gender', 'dob', 'dod', 'dod_hosp', 'dod_ssn',\n",
       "       'expire_flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUBJECT_ID**\n",
    "SUBJECT_ID is a unique identifier which specifies an individual patient. SUBJECT_ID is a candidate key for the table, so is unique for each row. Information that is consistent for the lifetime of a patient is stored in this table.\n",
    "\n",
    "**GENDER**\n",
    "GENDER is the genotypical sex of the patient.\n",
    "\n",
    "**DOB**\n",
    "DOB is the date of birth of the given patient. Patients who are older than 89 years old at any time in the database have had their date of birth shifted to obscure their age and comply with HIPAA. The shift process was as follows: the patient’s age at their first admission was determined. The date of birth was then set to exactly 300 years before their first admission.\n",
    "\n",
    "**DOD, DOD_HOSP, DOD_SSN**\n",
    "DOD is the date of death for the given patient. DOD_HOSP is the date of death as recorded in the hospital database. DOD_SSN is the date of death from the social security database. Note that DOD merged together DOD_HOSP and DOD_SSN, giving priority to DOD_HOSP if both were recorded.\n",
    "\n",
    "**EXPIRE_FLAG**\n",
    "EXPIRE_FLAG is a binary flag which indicates whether the patient died, i.e. whether DOD is null or not. These deaths include both deaths within the hospital (DOD_HOSP) and deaths identified by matching the patient to the social security master death index (DOD_SSN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_id', 'subject_id', 'hadm_id', 'transfertime', 'prev_service',\n",
       "       'curr_service'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "services.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUBJECT_ID, HADM_ID**\n",
    "Identifiers which specify the patient: SUBJECT_ID is unique to a patient and HADM_ID is unique to a patient hospital stay.\n",
    "\n",
    "**TRANSFERTIME**\n",
    "TRANSFERTIME is the time at which the patient moved from the PREV_SERVICE (if present) to the CURR_SERVICE.\n",
    "\n",
    "**PREV_SERVICE, CURR_SERVICE**\n",
    "PREV_SERVICE and CURR_SERVICE are the previous and current service that the patient resides under.\n",
    "\n",
    "\n",
    "**Service\tDescription**\n",
    "\n",
    "**CMED**\tCardiac Medical - for non-surgical cardiac related admissions\n",
    "\n",
    "**CSURG**\tCardiac Surgery - for surgical cardiac admissions\n",
    "\n",
    "**DENT**\tDental - for dental/jaw related admissions\n",
    "\n",
    "**ENT**\tEar, nose, and throat - conditions primarily affecting these areas\n",
    "\n",
    "**GU**\tGenitourinary - reproductive organs/urinary system\n",
    "\n",
    "**GYN**\tGynecological - female reproductive systems and breasts\n",
    "\n",
    "**MED**\tMedical - general service for internal medicine\n",
    "\n",
    "**NB**\tNewborn - infants born at the hospital\n",
    "\n",
    "**NBB**\tNewborn baby - infants born at the hospital\n",
    "\n",
    "**NMED**\tNeurologic Medical - non-surgical, relating to the brain\n",
    "\n",
    "**NSURG**\tNeurologic Surgical - surgical, relating to the brain\n",
    "\n",
    "**OBS**\tObstetrics - conerned with childbirth and the care of women giving birth\n",
    "\n",
    "**ORTHO**\tOrthopaedic - surgical, relating to the musculoskeletal system\n",
    "\n",
    "**OMED**\tOrthopaedic medicine - non-surgical, relating to musculoskeletal system\n",
    "\n",
    "**PSURG**\tPlastic - restortation/reconstruction of the human body (including cosmetic or aesthetic)\n",
    "\n",
    "**PSYCH**\tPsychiatric - mental disorders relating to mood, behaviour, cognition, or perceptions\n",
    "\n",
    "**SURG**\tSurgical - general surgical service not classified elsewhere\n",
    "\n",
    "**TRAUM**\tTrauma - injury or damage caused by physical harm from an external source\n",
    "\n",
    "**TSURG**\tThoracic Surgical - surgery on the thorax, located between the neck and the abdomen\n",
    "\n",
    "**VSURG**\tVascular Surgical - surgery relating to the circulatory system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_id', 'subject_id', 'hadm_id', 'icustay_id', 'startdate', 'enddate',\n",
       "       'drug_type', 'drug', 'drug_name_poe', 'drug_name_generic',\n",
       "       'formulary_drug_cd', 'gsn', 'ndc', 'prod_strength', 'dose_val_rx',\n",
       "       'dose_unit_rx', 'form_val_disp', 'form_unit_disp', 'route'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prescriptions.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUBJECT_ID, HADM_ID, ICUSTAY_ID**\n",
    "Identifiers which specify the patient: SUBJECT_ID is unique to a patient, HADM_ID is unique to a patient hospital stay and ICUSTAY_ID is unique to a patient ICU stay.\n",
    "\n",
    "**STARTDATE, ENDDATE**\n",
    "STARTDATE and ENDDATE specify the date period for which the prescription was valid.\n",
    "\n",
    "**DRUG_TYPE**\n",
    "DRUG_TYPE provides the type of drug prescribed.\n",
    "\n",
    "**DRUG, DRUG_NAME_POE, DRUG_NAME_GENERIC**\n",
    "These columns are various representations of the drug prescribed to the patient.\n",
    "\n",
    "**FORMULARY_DRUG_CD, GSN, NDC**\n",
    "These columns provide a representation of the drug in various coding systems. GSN is the Generic Sequence Number. NDC is the National Drug Code\n",
    "\n",
    "**PROD_STRENGTH\n",
    "DOSE_VAL_RX, DOSE_UNIT_RX\n",
    "FORM_VAL_DISP, FORM_UNIT_DISP**\n",
    "\n",
    "**ROUTE**\n",
    "The route prescribed for the drug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get unique values\n",
    "def unique(list1):\n",
    "    x = np.array(list1)\n",
    "    print(np.unique(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dataset: phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the interested outcome feature\n",
    "phenotypes_reduced = phenotypes[['hadm_id','subject_id','chronic.pain.fibromyalgia']]\n",
    "# Drop duplicated records by subject_id and hadm_id\n",
    "phenotypes_reduced = phenotypes_reduced.drop_duplicates(subset=['subject_id','hadm_id'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813, 3)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of phenotypes_reduced\n",
    "phenotypes_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>chronic.pain.fibromyalgia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100103</td>\n",
       "      <td>3365</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100137</td>\n",
       "      <td>27290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100473</td>\n",
       "      <td>5525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100485</td>\n",
       "      <td>41515</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100548</td>\n",
       "      <td>2265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hadm_id subject_id chronic.pain.fibromyalgia\n",
       "0  100103       3365                         0\n",
       "1  100137      27290                         0\n",
       "2  100473       5525                         0\n",
       "3  100485      41515                         0\n",
       "4  100548       2265                         0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phenotypes_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1979\n",
       "1     297\n",
       "Name: chronic.pain.fibromyalgia, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phenotypes['chronic.pain.fibromyalgia'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dataset: admissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding diagnosis feature from admission: \n",
    "\n",
    "15,693 distinct diagnoses for 58,976 admissions. The diagnoses can be very informative (e.g. chronic kidney failure) or quite vague (e.g. weakness). Final diagnoses for a patient’s hospital stay are coded on discharge and can be found in the DIAGNOSES_ICD table. While this field can provide information about the status of a patient on hospital admission, it is not recommended to use it to stratify patients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1944, 19)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of admissions_reduced\n",
    "admissions_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the dates and times from the database are deidentified -- create new features to get the time difference\n",
    "admissions_reduced['edouttime'] = pd.to_datetime(admissions_reduced['edouttime'])\n",
    "admissions_reduced['edregtime'] = pd.to_datetime(admissions_reduced['edregtime'])\n",
    "admissions_reduced['length_ed'] = (admissions_reduced['edouttime'] - admissions_reduced['edregtime']).dt.days\n",
    "admissions_reduced['dischtime'] = pd.to_datetime(admissions_reduced['dischtime'])\n",
    "admissions_reduced['admittime'] = pd.to_datetime(admissions_reduced['admittime'])\n",
    "admissions_reduced['length_admit'] = (admissions_reduced['dischtime'] - admissions_reduced['admittime']).dt.days\n",
    "# Drop time-related features used to create new features\n",
    "admissions_reduced = admissions_reduced.drop(['edregtime', 'edouttime', 'dischtime', 'admittime'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregate, dummy, and new variables for admission df to create one row per id\n",
    "just_dummies = pd.get_dummies(admissions_reduced['admission_type'], prefix='admission_type', drop_first=True)\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for admission location\n",
    "just_dummies = pd.get_dummies(admissions_reduced['admission_location'], prefix='admission_loc', drop_first=True)\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for discharge location\n",
    "just_dummies = pd.get_dummies(admissions_reduced['discharge_location'], prefix='discharge_loc', drop_first=True)\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for insurance\n",
    "just_dummies = pd.get_dummies(admissions_reduced['insurance'], prefix='insurance', drop_first=True)\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for religions\n",
    "just_dummies = pd.get_dummies(admissions_reduced['religion'], prefix='religion', drop_first=True)\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for language\n",
    "just_dummies = pd.get_dummies(admissions_reduced['language'], prefix='language', drop_first=True)\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for marital_status\n",
    "just_dummies = pd.get_dummies(admissions_reduced['marital_status'], prefix='marital_status', drop_first=True)\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for ethnicity\n",
    "just_dummies = pd.get_dummies(admissions_reduced['ethnicity'], prefix='ethnicity', drop_first=True)\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features used to dummy variables\n",
    "admissions_reduced = admissions_reduced.drop(['row_id', 'deathtime', 'diagnosis', 'religion', 'language','marital_status', 'ethnicity', 'insurance', 'admission_location', 'discharge_location', 'admission_type'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions_reduced[\"hospital_expire_flag\"] = admissions_reduced.hospital_expire_flag.astype(float)\n",
    "admissions_reduced[\"has_chartevents_data\"] = admissions_reduced.has_chartevents_data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1944, 90)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of cleaned admissions_reduced dataset\n",
    "admissions_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dataset: patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473, 8)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of patients_reduced dataset\n",
    "patients_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numerical code for string variables in the gender feature\n",
    "patients_reduced.gender[patients_reduced.gender == 'm'] = 1\n",
    "patients_reduced.gender[patients_reduced.gender == 'f'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing PIH features that had been deidentified\n",
    "patients_reduced = patients_reduced.drop(['row_id', 'dob', 'dod', 'dod_hosp', 'dod_ssn'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>expire_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>690</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject_id gender expire_flag\n",
       "0        690      1           1\n",
       "1        704      1           1"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final features left for patients_reduced dataset\n",
    "patients_reduced.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning dataset:  drgcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3883, 8)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drgcodes_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>drg_type</th>\n",
       "      <th>drg_code</th>\n",
       "      <th>description</th>\n",
       "      <th>drg_severity</th>\n",
       "      <th>drg_mortality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>383</td>\n",
       "      <td>6451</td>\n",
       "      <td>183196</td>\n",
       "      <td>hcfa</td>\n",
       "      <td>416</td>\n",
       "      <td>septicemia age &gt;17</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>432</td>\n",
       "      <td>4655</td>\n",
       "      <td>143283</td>\n",
       "      <td>hcfa</td>\n",
       "      <td>304</td>\n",
       "      <td>kidney, ureter &amp; major bladder procedures for ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>23000</td>\n",
       "      <td>132906</td>\n",
       "      <td>hcfa</td>\n",
       "      <td>258</td>\n",
       "      <td>total mastectomy for malignancy without compli...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id subject_id hadm_id drg_type drg_code  \\\n",
       "0    383       6451  183196     hcfa      416   \n",
       "1    432       4655  143283     hcfa      304   \n",
       "2     26      23000  132906     hcfa      258   \n",
       "\n",
       "                                         description drg_severity  \\\n",
       "0                                 septicemia age >17          nan   \n",
       "1  kidney, ureter & major bladder procedures for ...          nan   \n",
       "2  total mastectomy for malignancy without compli...          nan   \n",
       "\n",
       "  drg_mortality  \n",
       "0           nan  \n",
       "1           nan  \n",
       "2           nan  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drgcodes_reduced.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for drg_code\n",
    "just_dummies = pd.get_dummies(drgcodes_reduced['drg_code'], prefix='drg_code', drop_first=True)\n",
    "drgcodes_reduced = pd.concat([drgcodes_reduced, just_dummies], axis=1)\n",
    "\n",
    "# Create dummy variables for drg_code\n",
    "just_dummies = pd.get_dummies(drgcodes_reduced['drg_type'], prefix='drg_type', drop_first=True)\n",
    "drgcodes_reduced = pd.concat([drgcodes_reduced, just_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform object to numerical features\n",
    "drgcodes_reduced['drg_mortality'] = pd.to_numeric(drgcodes_reduced.drg_mortality, errors='coerce').fillna(0, downcast='infer').astype('Int32')\n",
    "drgcodes_reduced['drg_severity'] = pd.to_numeric(drgcodes_reduced.drg_severity, errors='coerce').fillna(0, downcast='infer').astype('Int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to have one record for each unique combination of subject_id and hadm_id, mean of the drg_mortality and drg_severity\n",
    "# are calculated\n",
    "drgcodes_reduced['avg_drg_mortality'] = drgcodes_reduced.groupby(['subject_id', 'hadm_id']).drg_mortality.transform('mean')\n",
    "drgcodes_reduced['avg_drg_severity'] = drgcodes_reduced.groupby(['subject_id', 'hadm_id']).drg_severity.transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "drgcodes_reduced['avg_drg_mortality'] = drgcodes_reduced.avg_drg_mortality.astype(float)\n",
    "drgcodes_reduced['avg_drg_severity'] = drgcodes_reduced.avg_drg_severity.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates by comparing subject_id and hadm_id\n",
    "drgcodes_reduced = drgcodes_reduced.drop_duplicates(subset=['subject_id','hadm_id'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "drgcodes_reduced = drgcodes_reduced.drop(['row_id', 'description', 'drg_code', 'drg_type', 'drg_severity', 'drg_mortality' ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1941, 641)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final size of the drgcodes_reduced\n",
    "drgcodes_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject_id', 'hadm_id', 'drg_code_10', 'drg_code_100', 'drg_code_101',\n",
       "       'drg_code_104', 'drg_code_105', 'drg_code_106', 'drg_code_107',\n",
       "       'drg_code_108',\n",
       "       ...\n",
       "       'drg_code_97', 'drg_code_974', 'drg_code_977', 'drg_code_981',\n",
       "       'drg_code_987', 'drg_code_99', 'drg_type_hcfa', 'drg_type_ms',\n",
       "       'avg_drg_mortality', 'avg_drg_severity'],\n",
       "      dtype='object', length=641)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drgcodes_reduced.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813, 3)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phenotypes_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main = pd.merge(admissions_reduced, phenotypes_reduced,\n",
    "                how ='right',\n",
    "                on = ['subject_id', 'hadm_id'])\n",
    "\n",
    "main = pd.merge(main, patients_reduced,\n",
    "                how ='left',\n",
    "                on = ['subject_id'])\n",
    "\n",
    "main = pd.merge(main, drgcodes_reduced,\n",
    "                how ='left',\n",
    "                on = ['subject_id', 'hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "main[\"expire_flag\"] = main.expire_flag.astype(float)\n",
    "main['gender'] = main.gender.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813, 732)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.shape # final dataset (813, 742)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.to_csv('data/main.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missingness of the final merged datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id                                  0.000000\n",
       "hadm_id                                     0.000000\n",
       "hospital_expire_flag                        0.000000\n",
       "has_chartevents_data                        0.000000\n",
       "length_ed                                   0.303813\n",
       "length_admit                                0.000000\n",
       "admission_type_elective                     0.000000\n",
       "admission_type_emergency                    0.000000\n",
       "admission_type_urgent                       0.000000\n",
       "admission_loc_clinic referral/premature     0.000000\n",
       "admission_loc_emergency room admit          0.000000\n",
       "admission_loc_phys referral/normal deli     0.000000\n",
       "admission_loc_transfer from hosp/extram     0.000000\n",
       "admission_loc_transfer from other healt     0.000000\n",
       "admission_loc_transfer from skilled nur     0.000000\n",
       "discharge_loc_dead/expired                  0.000000\n",
       "discharge_loc_disc-tran cancer/chldrn h     0.000000\n",
       "discharge_loc_disch-tran to psych hosp      0.000000\n",
       "discharge_loc_home                          0.000000\n",
       "discharge_loc_home health care              0.000000\n",
       "discharge_loc_home with home iv providr     0.000000\n",
       "discharge_loc_hospice-home                  0.000000\n",
       "discharge_loc_hospice-medical facility      0.000000\n",
       "discharge_loc_icf                           0.000000\n",
       "discharge_loc_left against medical advi     0.000000\n",
       "discharge_loc_long term care hospital       0.000000\n",
       "discharge_loc_other facility                0.000000\n",
       "discharge_loc_rehab/distinct part hosp      0.000000\n",
       "discharge_loc_short term hospital           0.000000\n",
       "discharge_loc_snf                           0.000000\n",
       "insurance_government                        0.000000\n",
       "insurance_medicaid                          0.000000\n",
       "insurance_medicare                          0.000000\n",
       "insurance_private                           0.000000\n",
       "insurance_self pay                          0.000000\n",
       "religion_baptist                            0.000000\n",
       "religion_buddhist                           0.000000\n",
       "religion_catholic                           0.000000\n",
       "religion_christian scientist                0.000000\n",
       "religion_episcopalian                       0.000000\n",
       "religion_greek orthodox                     0.000000\n",
       "religion_hebrew                             0.000000\n",
       "religion_jehovah's witness                  0.000000\n",
       "religion_jewish                             0.000000\n",
       "religion_muslim                             0.000000\n",
       "religion_nan                                0.000000\n",
       "religion_not specified                      0.000000\n",
       "religion_other                              0.000000\n",
       "religion_protestant quaker                  0.000000\n",
       "religion_romanian east. orth                0.000000\n",
       "religion_unitarian-universalist             0.000000\n",
       "religion_unobtainable                       0.000000\n",
       "language_*chi                               0.000000\n",
       "language_*hun                               0.000000\n",
       "language_*man                               0.000000\n",
       "language_arab                               0.000000\n",
       "language_camb                               0.000000\n",
       "language_cant                               0.000000\n",
       "language_cape                               0.000000\n",
       "language_engl                               0.000000\n",
       "language_fren                               0.000000\n",
       "language_gree                               0.000000\n",
       "language_hait                               0.000000\n",
       "language_ital                               0.000000\n",
       "language_nan                                0.000000\n",
       "language_pers                               0.000000\n",
       "language_port                               0.000000\n",
       "language_ptun                               0.000000\n",
       "language_russ                               0.000000\n",
       "language_span                               0.000000\n",
       "language_urdu                               0.000000\n",
       "marital_status_divorced                     0.000000\n",
       "marital_status_life partner                 0.000000\n",
       "marital_status_married                      0.000000\n",
       "marital_status_nan                          0.000000\n",
       "marital_status_separated                    0.000000\n",
       "marital_status_single                       0.000000\n",
       "marital_status_unknown (default)            0.000000\n",
       "marital_status_widowed                      0.000000\n",
       "ethnicity_asian                             0.000000\n",
       "ethnicity_asian - asian indian              0.000000\n",
       "ethnicity_asian - chinese                   0.000000\n",
       "ethnicity_black/african                     0.000000\n",
       "ethnicity_black/african american            0.000000\n",
       "ethnicity_black/cape verdean                0.000000\n",
       "ethnicity_black/haitian                     0.000000\n",
       "ethnicity_hispanic or latino                0.000000\n",
       "ethnicity_hispanic/latino - guatemalan      0.000000\n",
       "ethnicity_hispanic/latino - puerto rican    0.000000\n",
       "ethnicity_other                             0.000000\n",
       "ethnicity_patient declined to answer        0.000000\n",
       "ethnicity_portuguese                        0.000000\n",
       "ethnicity_unable to obtain                  0.000000\n",
       "ethnicity_unknown/not specified             0.000000\n",
       "ethnicity_white                             0.000000\n",
       "ethnicity_white - brazilian                 0.000000\n",
       "ethnicity_white - eastern european          0.000000\n",
       "ethnicity_white - russian                   0.000000\n",
       "chronic.pain.fibromyalgia                   0.000000\n",
       "gender                                      0.000000\n",
       "expire_flag                                 0.000000\n",
       "avg_drg_mortality                           0.000000\n",
       "avg_drg_severity                            0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.isnull().mean() # length_ed had ~30% missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "main['length_ed'] = main['length_ed'].fillna(0) # not entirely sure about this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructuring -- Move the outcome variable to be the last column in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(main.columns.values)\n",
    "cols.pop(cols.index('chronic.pain.fibromyalgia'))\n",
    "main = main[cols+['chronic.pain.fibromyalgia']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "main[\"hospital_expire_flag\"] = main.hospital_expire_flag.astype(float)\n",
    "main[\"has_chartevents_data\"] = main.has_chartevents_data.astype(float)\n",
    "main[\"expire_flag\"] = main.expire_flag.astype(float)\n",
    "main['gender'] = main.gender.astype(int)\n",
    "main['avg_drg_mortality'] = main.avg_drg_mortality.astype(float)\n",
    "main['avg_drg_severity'] = main.avg_drg_severity.astype(float)\n",
    "main['chronic.pain.fibromyalgia'] = main['chronic.pain.fibromyalgia'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set X as features and y as the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = main.drop(['subject_id', 'hadm_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High correlation filter\n",
    "\n",
    "This dimensionality reduction algorithm tries to discard inputs that are very similar to others. In simple words, if your opinion is same as your boss, one of you is not required. If the value of two input parameters is always the same, it means they represent the same entity. Then we do not need two parameters there. Just one should be enough.\n",
    "\n",
    "In technical words, if there is a very high correlation between two input variables, we can safely drop one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Absolute Correlations\n",
      "language_hait             ethnicity_black/haitian                    1.000000\n",
      "hospital_expire_flag      discharge_loc_dead/expired                 1.000000\n",
      "avg_drg_mortality         avg_drg_severity                           0.955766\n",
      "religion_buddhist         language_camb                              0.865491\n",
      "admission_type_elective   admission_type_emergency                   0.851231\n",
      "language_engl             language_nan                               0.824003\n",
      "language_port             ethnicity_white - brazilian                0.815993\n",
      "admission_type_emergency  admission_loc_phys referral/normal deli    0.811482\n",
      "admission_type_elective   admission_loc_phys referral/normal deli    0.810679\n",
      "language_cape             ethnicity_black/cape verdean               0.706671\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]\n",
    "\n",
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(df, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing highly correlated features\n",
    "main = main.drop(['language_hait', 'discharge_loc_dead/expired'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_drg_mortality         avg_drg_severity                           0.955766\n",
      "religion_buddhist         language_camb                              0.865491\n",
      "admission_type_elective   admission_type_emergency                   0.851231\n",
      "language_engl             language_nan                               0.824003\n",
      "language_port             ethnicity_white - brazilian                0.815993\n",
      "admission_type_emergency  admission_loc_phys referral/normal deli    0.811482\n",
      "admission_type_elective   admission_loc_phys referral/normal deli    0.810679\n",
      "language_cape             ethnicity_black/cape verdean               0.706671\n",
      "religion_muslim           language_urdu                              0.705796\n",
      "insurance_medicare        insurance_private                          0.697338\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(get_top_abs_correlations(main, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = main['depression']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813, 98)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = main.drop(['depression'], axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXgA6CzlqbCl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling - Standardization vs. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is essential for machine learning algorithms that calculate distances between data. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\n",
    "\n",
    "* Normalization is recommended when you have a normally distributed observations.\n",
    "* Standardization works all the time. (recommended)\n",
    "* We need to perform Feature Scaling when we are dealing with Gradient Descent Based algorithms (Linear and Logistic Regression, Neural Network) and Distance-based algorithms (KNN, K-means, SVM) as these are very sensitive to the range of the data points.\n",
    "\n",
    "* It is a good practice to fit the scaler on the training data and then use it to transform the testing data. This would avoid any data leakage during the model testing process. Also, the scaling of target values is generally not required.\n",
    "* Only apply standardization to numerical columns and not the other One-Hot Encoded features. Standardizing the One-Hot encoded features would mean assigning a distribution to categorical features. You don’t want to do that! While it is fine to apply normalization to all kinds of columns including One-Hot Encorded features because One-Hot encoded features are already in the range between 0 to 1. So, normalization would not affect their value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# fit scaler on training data\n",
    "norm = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# transform training data\n",
    "X_train_norm = norm.transform(X_train)\n",
    "\n",
    "# transofrm teting data\n",
    "X_test_norm = norm.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization with sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# copy of datasets\n",
    "X_train_stand = X_train.copy()\n",
    "X_test_stand = X_test.copy()\n",
    "\n",
    "# numerical features\n",
    "num_cols = ['length_admit', 'avg_drg_mortality', 'avg_drg_severity']\n",
    "\n",
    "# apply standardization on numerical features\n",
    "for i in num_cols:\n",
    "    scale = StandardScaler().fit(X_train_stand[[i]])\n",
    "    X_train_stand[i] = scale.transform(X_train_stand[[i]])\n",
    "    X_test_stand[i] = scale.transform(X_test_stand[[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction is a data preparation technique performed on data prior to modeling. It might be performed after data cleaning and data scaling and before training a predictive model.\n",
    "\n",
    "As such, any dimensionality reduction performed on training data must also be performed on new data, such as a test dataset, validation dataset, and data when making a prediction with the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "Standardization: All the variables should be on the same scale before applying PCA, otherwise, a feature with large values will dominate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 10)\n",
    "X_train_norm_pca = pca.fit_transform(X_train_norm)\n",
    "X_test_norm_pca = pca.transform(X_test_norm)\n",
    "X_train_stand_pca = pca.fit_transform(X_train_stand)\n",
    "X_test_stand_pca = pca.transform(X_test_stand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[132   3]\n",
      " [ 26   2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8220858895705522"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test_norm_pca)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[133   2]\n",
      " [ 27   1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8220858895705522"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train_stand_pca, y_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test_stand_pca)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[120  15]\n",
      " [ 18  10]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7975460122699386"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) # for Euclidean distance\n",
    "classifier.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "# predicting the test set results\n",
    "y_pred = classifier.predict(X_test_norm_pca)\n",
    "\n",
    "# making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124  11]\n",
      " [ 21   7]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.803680981595092"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) # for Euclidean distance\n",
    "classifier.fit(X_train_stand_pca, y_train)\n",
    "\n",
    "# predicting the test set results\n",
    "y_pred = classifier.predict(X_test_stand_pca)\n",
    "\n",
    "# making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption: observations are linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[135   0]\n",
      " [ 28   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8282208588957055"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[135   0]\n",
      " [ 28   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8282208588957055"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train_stand_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maaping to a higher dimensional space can be highly compute-intensive.\n",
    "\n",
    "Types of Kernal Functions:\n",
    "* Gaussian RBF Kernel\n",
    "* Sigmoid Kernel\n",
    "* Polynomial Kernel\n",
    "* and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[135   0]\n",
      " [ 28   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8282208588957055"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[135   0]\n",
      " [ 28   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8282208588957055"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train_stand_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages: This algorithm requires a small amount of training data to estimate the necessary parameters. Naive Bayes classifiers are extremely fast compared to more sophisticated methods.\n",
    "\n",
    "Disadvantages: Naive Bayes is is known to be a bad estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[130   5]\n",
      " [ 26   2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8098159509202454"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[131   4]\n",
      " [ 26   2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8159509202453987"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train_stand_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages: Decision Tree is simple to understand and visualise, requires little data preparation, and can handle both numerical and categorical data.\n",
    "\n",
    "Disadvantages: Decision tree can create complex trees that do not generalise well, and decision trees can be unstable because small variations in the data might result in a completely different tree being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[108  27]\n",
      " [ 18  10]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7239263803680982"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[95 40]\n",
      " [14 14]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6687116564417178"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train_stand_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Learning : using different machine algorithms \n",
    "\n",
    "The algorithm does not work well for datasets having a lot of outliers, something which needs addressing prior to the model building.\n",
    "\n",
    "Advantages: Reduction in over-fitting and random forest classifier is more accurate than decision trees in most cases.\n",
    "\n",
    "Disadvantages: Slow real time prediction, difficult to implement, and complex algorithm.\n",
    "\n",
    "Build on top of Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[126   9]\n",
      " [ 23   5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.803680981595092"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[127   8]\n",
      " [ 25   3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7975460122699386"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train_stand_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# fit scaler on training data\n",
    "norm = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# transform training data\n",
    "X_train_norm = norm.transform(X_train)\n",
    "\n",
    "# transofrm teting data\n",
    "X_test_norm = norm.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization with sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# copy of datasets\n",
    "X_train_stand = X_train.copy()\n",
    "X_test_stand = X_test.copy()\n",
    "\n",
    "# numerical features\n",
    "num_cols = ['length_admit', 'avg_drg_mortality', 'avg_drg_severity']\n",
    "\n",
    "# apply standardization on numerical features\n",
    "for i in num_cols:\n",
    "    scale = StandardScaler().fit(X_train_stand[[i]])\n",
    "    X_train_stand[i] = scale.transform(X_train_stand[[i]])\n",
    "    X_test_stand[i] = scale.transform(X_test_stand[[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components = 1)\n",
    "X_train_norm_lda = lda.fit_transform(X_train_norm, y_train)\n",
    "X_test_norm_lda = lda.transform(X_test_norm)\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components = 1)\n",
    "X_train_stand_lda = lda.fit_transform(X_train_stand, y_train)\n",
    "X_test_stand_lda = lda.transform(X_test_stand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[119  16]\n",
      " [ 23   5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7607361963190185"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train_norm_lda, y_train)\n",
    "\n",
    "# predicting the test set results\n",
    "y_pred = classifier.predict(X_test_norm_lda)\n",
    "\n",
    "# Making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[119  16]\n",
      " [ 23   5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7607361963190185"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train_stand_lda, y_train)\n",
    "\n",
    "# predicting the test set results\n",
    "y_pred = classifier.predict(X_test_stand_lda)\n",
    "\n",
    "# Making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbor (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbor (KNN) algorithm predicts based on the specified number (k) of the nearest neighboring data points. Here, the pre-processing of the data is significant as it impacts the distance measurements directly. Unlike others, the model does not have a mathematical formula, neither any descriptive ability.\n",
    "\n",
    "It is a simple, fairly accurate model preferable mostly for smaller datasets, owing to huge computations involved on the continuous predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Choose the number of K of neighbors\n",
    "\n",
    "Step 2: Take the K nearest neighbors of the new data point, according to the Euclidean distance\n",
    "\n",
    "Step 3: Among these K neighbors, count the number of data points in each category\n",
    "\n",
    "Step 4: Assign the new data point ot the category where you counted the most neighbors\n",
    "\n",
    "Then, the model will be ready. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[119  16]\n",
      " [ 22   6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7668711656441718"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) # for Euclidean distance\n",
    "classifier.fit(X_train_norm_lda, y_train)\n",
    "\n",
    "# predicting the test set results\n",
    "y_pred = classifier.predict(X_test_norm_lda)\n",
    "\n",
    "# making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[119  16]\n",
      " [ 22   6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7668711656441718"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) # for Euclidean distance\n",
    "classifier.fit(X_train_stand_lda, y_train)\n",
    "\n",
    "# predicting the test set results\n",
    "y_pred = classifier.predict(X_test_stand_lda)\n",
    "\n",
    "# making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption: observations are linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[127   8]\n",
      " [ 25   3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7975460122699386"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train_norm_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[127   8]\n",
      " [ 25   3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7975460122699386"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train_stand_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maaping to a higher dimensional space can be highly compute-intensive.\n",
    "\n",
    "Types of Kernal Functions:\n",
    "* Gaussian RBF Kernel\n",
    "* Sigmoid Kernel\n",
    "* Polynomial Kernel\n",
    "* and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[127   8]\n",
      " [ 25   3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7975460122699386"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train_norm_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[127   8]\n",
      " [ 25   3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7975460122699386"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train_stand_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages: This algorithm requires a small amount of training data to estimate the necessary parameters. Naive Bayes classifiers are extremely fast compared to more sophisticated methods.\n",
    "\n",
    "Disadvantages: Naive Bayes is is known to be a bad estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[119  16]\n",
      " [ 23   5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7607361963190185"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train_norm_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[119  16]\n",
      " [ 23   5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7607361963190185"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train_stand_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages: Decision Tree is simple to understand and visualise, requires little data preparation, and can handle both numerical and categorical data.\n",
    "\n",
    "Disadvantages: Decision tree can create complex trees that do not generalise well, and decision trees can be unstable because small variations in the data might result in a completely different tree being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[93 42]\n",
      " [17 11]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6380368098159509"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train_norm_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[93 42]\n",
      " [17 11]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6380368098159509"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train_stand_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Learning : using different machine algorithms \n",
    "\n",
    "The algorithm does not work well for datasets having a lot of outliers, something which needs addressing prior to the model building.\n",
    "\n",
    "Advantages: Reduction in over-fitting and random forest classifier is more accurate than decision trees in most cases.\n",
    "\n",
    "Disadvantages: Slow real time prediction, difficult to implement, and complex algorithm.\n",
    "\n",
    "Build on top of Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[97 38]\n",
      " [19  9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6503067484662577"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train_norm_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[97 38]\n",
      " [19  9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6503067484662577"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train_stand_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "1. Try GridSearch with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing_tools.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
