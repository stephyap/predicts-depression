{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37puETfgRzzg"
   },
   "source": [
    "# To Predict Depression Among Patients Admitted to ICU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "## Part I: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:95% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adjust notebook settings to widen the notebook\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:95% !important;}</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-qiINBQSK2g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "### Import modules/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwEPNDWySTKm"
   },
   "outputs": [],
   "source": [
    "# every unique hospitalization for each patient in the database (defines HADM_ID_\n",
    "admissions = pd.read_csv('data/ADMISSIONS.csv')\n",
    "# Diagnosis Related Groups (DRG), which are used by the hospital for billing purposes.\n",
    "drgcodes = pd.read_csv(\"data/DRGCODES.csv\")\n",
    "# Deidentified notes, including nursing and physician notes, ECG reports, imaging reports, and discharge summaries.\n",
    "noteevents = pd.read_csv(\"data/NOTEEVENTS.csv\")\n",
    "# every unique patient in the database (defines subject_id)\n",
    "patients = pd.read_csv(\"data/PATIENTS.csv\")\n",
    "# the clinical service under which a patient is registered\n",
    "services = pd.read_csv(\"data/SERVICES.csv\")\n",
    "# Medications ordered, and not necessarily administered, for a given patient\n",
    "prescriptions = pd.read_csv(\"data/PRESCRIPTIONS.csv\")\n",
    "# Ground truth dataset\n",
    "phenotypes = pd.read_csv(\"data/GROUND_TRUTH.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all strings inside of a dataframe to lowercase\n",
    "admissions = admissions.apply(lambda x: x.astype(str).str.lower())\n",
    "drgcodes = drgcodes.apply(lambda x: x.astype(str).str.lower())\n",
    "noteevents = noteevents.apply(lambda x: x.astype(str).str.lower())\n",
    "patients = patients.apply(lambda x: x.astype(str).str.lower())\n",
    "services = services.apply(lambda x: x.astype(str).str.lower())\n",
    "prescriptions = prescriptions.apply(lambda x: x.astype(str).str.lower())\n",
    "phenotypes = phenotypes.apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "# lowercase columns in all dataframes\n",
    "admissions.columns = admissions.columns.str.lower()\n",
    "drgcodes.columns = drgcodes.columns.str.lower()\n",
    "noteevents.columns = noteevents.columns.str.lower()\n",
    "patients.columns = patients.columns.str.lower()\n",
    "services.columns = services.columns.str.lower()\n",
    "prescriptions.columns = prescriptions.columns.str.lower()\n",
    "phenotypes.columns = phenotypes.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decrease the datasets by subsetting the records which ID is in phenotypes dataset\n",
    "admissions_reduced = admissions[admissions['subject_id'].isin(phenotypes['subject_id'])]\n",
    "drgcodes_reduced = drgcodes[drgcodes['subject_id'].isin(phenotypes['subject_id'])]\n",
    "noteevents_reduced = noteevents[noteevents['subject_id'].isin(phenotypes['subject_id'])]\n",
    "patients_reduced = patients[patients['subject_id'].isin(phenotypes['subject_id'])]\n",
    "services_reduced = services[services['subject_id'].isin(phenotypes['subject_id'])]\n",
    "prescriptions_reduced = prescriptions[prescriptions['subject_id'].isin(phenotypes['subject_id'])]\n",
    "\n",
    "admissions_reduced = admissions_reduced.reset_index(drop=True)\n",
    "drgcodes_reduced = drgcodes_reduced.reset_index(drop=True)\n",
    "noteevents_reduced = noteevents_reduced.reset_index(drop=True)\n",
    "patients_reduced = patients_reduced.reset_index(drop=True)\n",
    "services_reduced = services_reduced.reset_index(drop=True)\n",
    "prescriptions_reduced = prescriptions_reduced.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get unique values\n",
    "def unique(list1):\n",
    "    x = np.array(list1)\n",
    "    print(np.unique(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dataset: phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the interested outcome feature\n",
    "phenotypes_reduced = phenotypes[['hadm_id','subject_id','depression']]\n",
    "# Drop duplicated records by subject_id and hadm_id\n",
    "phenotypes_reduced = phenotypes_reduced.drop_duplicates(subset=['subject_id','hadm_id'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of phenotypes_reduced\n",
    "phenotypes_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100103</td>\n",
       "      <td>3365</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100137</td>\n",
       "      <td>27290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100473</td>\n",
       "      <td>5525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100485</td>\n",
       "      <td>41515</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100548</td>\n",
       "      <td>2265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hadm_id subject_id depression\n",
       "0  100103       3365          0\n",
       "1  100137      27290          0\n",
       "2  100473       5525          0\n",
       "3  100485      41515          1\n",
       "4  100548       2265          0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phenotypes_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dataset: admissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding diagnosis feature from admission: \n",
    "\n",
    "15,693 distinct diagnoses for 58,976 admissions. The diagnoses can be very informative (e.g. chronic kidney failure) or quite vague (e.g. weakness). Final diagnoses for a patientâ€™s hospital stay are coded on discharge and can be found in the DIAGNOSES_ICD table. While this field can provide information about the status of a patient on hospital admission, it is not recommended to use it to stratify patients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1944, 19)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of admissions_reduced\n",
    "admissions_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the dates and times from the database are deidentified -- create new features to get the time difference\n",
    "admissions_reduced['edouttime'] = pd.to_datetime(admissions_reduced['edouttime'])\n",
    "admissions_reduced['edregtime'] = pd.to_datetime(admissions_reduced['edregtime'])\n",
    "admissions_reduced['length_ed'] = (admissions_reduced['edouttime'] - admissions_reduced['edregtime']).dt.days\n",
    "admissions_reduced['dischtime'] = pd.to_datetime(admissions_reduced['dischtime'])\n",
    "admissions_reduced['admittime'] = pd.to_datetime(admissions_reduced['admittime'])\n",
    "admissions_reduced['length_admit'] = (admissions_reduced['dischtime'] - admissions_reduced['admittime']).dt.days\n",
    "# Drop time-related features used to create new features\n",
    "admissions_reduced = admissions_reduced.drop(['edregtime', 'edouttime', 'dischtime', 'admittime'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregate, dummy, and new variables for admission df to create one row per id\n",
    "just_dummies = pd.get_dummies(admissions_reduced['admission_type'], prefix='admission_type')\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for admission location\n",
    "just_dummies = pd.get_dummies(admissions_reduced['admission_location'], prefix='admission_loc')\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for discharge location\n",
    "just_dummies = pd.get_dummies(admissions_reduced['discharge_location'], prefix='discharge_loc')\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for insurance\n",
    "just_dummies = pd.get_dummies(admissions_reduced['insurance'], prefix='insurance')\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for religions\n",
    "just_dummies = pd.get_dummies(admissions_reduced['religion'], prefix='religion')\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for language\n",
    "just_dummies = pd.get_dummies(admissions_reduced['language'], prefix='language')\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for marital_status\n",
    "just_dummies = pd.get_dummies(admissions_reduced['marital_status'], prefix='marital_status')\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)\n",
    "# Create dummy variables for ethnicity\n",
    "just_dummies = pd.get_dummies(admissions_reduced['ethnicity'], prefix='ethnicity')\n",
    "admissions_reduced = pd.concat([admissions_reduced, just_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features used to dummy variables\n",
    "admissions_reduced = admissions_reduced.drop(['row_id', 'deathtime', 'diagnosis', 'religion', 'language','marital_status', 'ethnicity', 'insurance', 'admission_location', 'discharge_location', 'admission_type'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "      <th>has_chartevents_data</th>\n",
       "      <th>length_ed</th>\n",
       "      <th>length_admit</th>\n",
       "      <th>admission_type_elective</th>\n",
       "      <th>admission_type_emergency</th>\n",
       "      <th>admission_type_urgent</th>\n",
       "      <th>admission_loc_clinic referral/premature</th>\n",
       "      <th>admission_loc_emergency room admit</th>\n",
       "      <th>admission_loc_phys referral/normal deli</th>\n",
       "      <th>admission_loc_transfer from hosp/extram</th>\n",
       "      <th>admission_loc_transfer from other healt</th>\n",
       "      <th>admission_loc_transfer from skilled nur</th>\n",
       "      <th>discharge_loc_dead/expired</th>\n",
       "      <th>discharge_loc_disc-tran cancer/chldrn h</th>\n",
       "      <th>discharge_loc_disch-tran to psych hosp</th>\n",
       "      <th>discharge_loc_home</th>\n",
       "      <th>discharge_loc_home health care</th>\n",
       "      <th>discharge_loc_home with home iv providr</th>\n",
       "      <th>discharge_loc_hospice-home</th>\n",
       "      <th>discharge_loc_hospice-medical facility</th>\n",
       "      <th>discharge_loc_icf</th>\n",
       "      <th>discharge_loc_left against medical advi</th>\n",
       "      <th>discharge_loc_long term care hospital</th>\n",
       "      <th>discharge_loc_other facility</th>\n",
       "      <th>discharge_loc_rehab/distinct part hosp</th>\n",
       "      <th>discharge_loc_short term hospital</th>\n",
       "      <th>discharge_loc_snf</th>\n",
       "      <th>insurance_government</th>\n",
       "      <th>insurance_medicaid</th>\n",
       "      <th>insurance_medicare</th>\n",
       "      <th>insurance_private</th>\n",
       "      <th>insurance_self pay</th>\n",
       "      <th>religion_baptist</th>\n",
       "      <th>religion_buddhist</th>\n",
       "      <th>religion_catholic</th>\n",
       "      <th>religion_christian scientist</th>\n",
       "      <th>religion_episcopalian</th>\n",
       "      <th>religion_greek orthodox</th>\n",
       "      <th>religion_hebrew</th>\n",
       "      <th>religion_jehovah's witness</th>\n",
       "      <th>religion_jewish</th>\n",
       "      <th>religion_muslim</th>\n",
       "      <th>religion_nan</th>\n",
       "      <th>religion_not specified</th>\n",
       "      <th>religion_other</th>\n",
       "      <th>religion_protestant quaker</th>\n",
       "      <th>religion_romanian east. orth</th>\n",
       "      <th>religion_unitarian-universalist</th>\n",
       "      <th>religion_unobtainable</th>\n",
       "      <th>language_*chi</th>\n",
       "      <th>language_*hun</th>\n",
       "      <th>language_*man</th>\n",
       "      <th>language_arab</th>\n",
       "      <th>language_camb</th>\n",
       "      <th>language_cant</th>\n",
       "      <th>language_cape</th>\n",
       "      <th>language_engl</th>\n",
       "      <th>language_fren</th>\n",
       "      <th>language_gree</th>\n",
       "      <th>language_hait</th>\n",
       "      <th>language_ital</th>\n",
       "      <th>language_nan</th>\n",
       "      <th>language_pers</th>\n",
       "      <th>language_port</th>\n",
       "      <th>language_ptun</th>\n",
       "      <th>language_russ</th>\n",
       "      <th>language_span</th>\n",
       "      <th>language_urdu</th>\n",
       "      <th>marital_status_divorced</th>\n",
       "      <th>marital_status_life partner</th>\n",
       "      <th>marital_status_married</th>\n",
       "      <th>marital_status_nan</th>\n",
       "      <th>marital_status_separated</th>\n",
       "      <th>marital_status_single</th>\n",
       "      <th>marital_status_unknown (default)</th>\n",
       "      <th>marital_status_widowed</th>\n",
       "      <th>ethnicity_asian</th>\n",
       "      <th>ethnicity_asian - asian indian</th>\n",
       "      <th>ethnicity_asian - chinese</th>\n",
       "      <th>ethnicity_black/african</th>\n",
       "      <th>ethnicity_black/african american</th>\n",
       "      <th>ethnicity_black/cape verdean</th>\n",
       "      <th>ethnicity_black/haitian</th>\n",
       "      <th>ethnicity_hispanic or latino</th>\n",
       "      <th>ethnicity_hispanic/latino - guatemalan</th>\n",
       "      <th>ethnicity_hispanic/latino - puerto rican</th>\n",
       "      <th>ethnicity_other</th>\n",
       "      <th>ethnicity_patient declined to answer</th>\n",
       "      <th>ethnicity_portuguese</th>\n",
       "      <th>ethnicity_unable to obtain</th>\n",
       "      <th>ethnicity_unknown/not specified</th>\n",
       "      <th>ethnicity_white</th>\n",
       "      <th>ethnicity_white - brazilian</th>\n",
       "      <th>ethnicity_white - eastern european</th>\n",
       "      <th>ethnicity_white - russian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>368</td>\n",
       "      <td>105889</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>368</td>\n",
       "      <td>138061</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject_id hadm_id hospital_expire_flag has_chartevents_data  length_ed  \\\n",
       "0        368  105889                    0                    1        0.0   \n",
       "1        368  138061                    0                    1        1.0   \n",
       "\n",
       "   length_admit  admission_type_elective  admission_type_emergency  \\\n",
       "0             4                        0                         1   \n",
       "1             5                        0                         1   \n",
       "\n",
       "   admission_type_urgent  admission_loc_clinic referral/premature  \\\n",
       "0                      0                                        0   \n",
       "1                      0                                        0   \n",
       "\n",
       "   admission_loc_emergency room admit  \\\n",
       "0                                   1   \n",
       "1                                   1   \n",
       "\n",
       "   admission_loc_phys referral/normal deli  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "\n",
       "   admission_loc_transfer from hosp/extram  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "\n",
       "   admission_loc_transfer from other healt  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "\n",
       "   admission_loc_transfer from skilled nur  discharge_loc_dead/expired  \\\n",
       "0                                        0                           0   \n",
       "1                                        0                           0   \n",
       "\n",
       "   discharge_loc_disc-tran cancer/chldrn h  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "\n",
       "   discharge_loc_disch-tran to psych hosp  discharge_loc_home  \\\n",
       "0                                       0                   0   \n",
       "1                                       0                   0   \n",
       "\n",
       "   discharge_loc_home health care  discharge_loc_home with home iv providr  \\\n",
       "0                               0                                        0   \n",
       "1                               0                                        0   \n",
       "\n",
       "   discharge_loc_hospice-home  discharge_loc_hospice-medical facility  \\\n",
       "0                           0                                       0   \n",
       "1                           0                                       0   \n",
       "\n",
       "   discharge_loc_icf  discharge_loc_left against medical advi  \\\n",
       "0                  0                                        0   \n",
       "1                  0                                        0   \n",
       "\n",
       "   discharge_loc_long term care hospital  discharge_loc_other facility  \\\n",
       "0                                      0                             0   \n",
       "1                                      0                             0   \n",
       "\n",
       "   discharge_loc_rehab/distinct part hosp  discharge_loc_short term hospital  \\\n",
       "0                                       0                                  0   \n",
       "1                                       0                                  0   \n",
       "\n",
       "   discharge_loc_snf  insurance_government  insurance_medicaid  \\\n",
       "0                  1                     0                   0   \n",
       "1                  1                     0                   0   \n",
       "\n",
       "   insurance_medicare  insurance_private  insurance_self pay  \\\n",
       "0                   1                  0                   0   \n",
       "1                   1                  0                   0   \n",
       "\n",
       "   religion_baptist  religion_buddhist  religion_catholic  \\\n",
       "0                 0                  0                  0   \n",
       "1                 0                  0                  0   \n",
       "\n",
       "   religion_christian scientist  religion_episcopalian  \\\n",
       "0                             0                      0   \n",
       "1                             0                      0   \n",
       "\n",
       "   religion_greek orthodox  religion_hebrew  religion_jehovah's witness  \\\n",
       "0                        0                0                           0   \n",
       "1                        0                0                           0   \n",
       "\n",
       "   religion_jewish  religion_muslim  religion_nan  religion_not specified  \\\n",
       "0                0                0             0                       1   \n",
       "1                0                0             0                       1   \n",
       "\n",
       "   religion_other  religion_protestant quaker  religion_romanian east. orth  \\\n",
       "0               0                           0                             0   \n",
       "1               0                           0                             0   \n",
       "\n",
       "   religion_unitarian-universalist  religion_unobtainable  language_*chi  \\\n",
       "0                                0                      0              0   \n",
       "1                                0                      0              0   \n",
       "\n",
       "   language_*hun  language_*man  language_arab  language_camb  language_cant  \\\n",
       "0              0              0              0              0              0   \n",
       "1              0              0              0              0              0   \n",
       "\n",
       "   language_cape  language_engl  language_fren  language_gree  language_hait  \\\n",
       "0              0              0              0              0              0   \n",
       "1              0              0              0              0              0   \n",
       "\n",
       "   language_ital  language_nan  language_pers  language_port  language_ptun  \\\n",
       "0              0             1              0              0              0   \n",
       "1              0             1              0              0              0   \n",
       "\n",
       "   language_russ  language_span  language_urdu  marital_status_divorced  \\\n",
       "0              0              0              0                        0   \n",
       "1              0              0              0                        0   \n",
       "\n",
       "   marital_status_life partner  marital_status_married  marital_status_nan  \\\n",
       "0                            0                       0                   0   \n",
       "1                            0                       0                   0   \n",
       "\n",
       "   marital_status_separated  marital_status_single  \\\n",
       "0                         0                      0   \n",
       "1                         0                      0   \n",
       "\n",
       "   marital_status_unknown (default)  marital_status_widowed  ethnicity_asian  \\\n",
       "0                                 0                       1                0   \n",
       "1                                 0                       1                0   \n",
       "\n",
       "   ethnicity_asian - asian indian  ethnicity_asian - chinese  \\\n",
       "0                               0                          0   \n",
       "1                               0                          0   \n",
       "\n",
       "   ethnicity_black/african  ethnicity_black/african american  \\\n",
       "0                        0                                 0   \n",
       "1                        0                                 0   \n",
       "\n",
       "   ethnicity_black/cape verdean  ethnicity_black/haitian  \\\n",
       "0                             0                        0   \n",
       "1                             0                        0   \n",
       "\n",
       "   ethnicity_hispanic or latino  ethnicity_hispanic/latino - guatemalan  \\\n",
       "0                             0                                       0   \n",
       "1                             0                                       0   \n",
       "\n",
       "   ethnicity_hispanic/latino - puerto rican  ethnicity_other  \\\n",
       "0                                         0                0   \n",
       "1                                         0                0   \n",
       "\n",
       "   ethnicity_patient declined to answer  ethnicity_portuguese  \\\n",
       "0                                     0                     0   \n",
       "1                                     0                     0   \n",
       "\n",
       "   ethnicity_unable to obtain  ethnicity_unknown/not specified  \\\n",
       "0                           0                                0   \n",
       "1                           0                                0   \n",
       "\n",
       "   ethnicity_white  ethnicity_white - brazilian  \\\n",
       "0                1                            0   \n",
       "1                1                            0   \n",
       "\n",
       "   ethnicity_white - eastern european  ethnicity_white - russian  \n",
       "0                                   0                          0  \n",
       "1                                   0                          0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions_reduced.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions_reduced[\"hospital_expire_flag\"] = admissions_reduced.hospital_expire_flag.astype(float)\n",
    "admissions_reduced[\"has_chartevents_data\"] = admissions_reduced.has_chartevents_data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1944, 98)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of cleaned admissions_reduced dataset\n",
    "admissions_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dataset: patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473, 8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of patients_reduced dataset\n",
    "patients_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numerical code for string variables in the gender feature\n",
    "patients_reduced.gender[patients_reduced.gender == 'm'] = 1\n",
    "patients_reduced.gender[patients_reduced.gender == 'f'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing PIH features that had been deidentified\n",
    "patients_reduced = patients_reduced.drop(['row_id', 'dob', 'dod', 'dod_hosp', 'dod_ssn'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>expire_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>690</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject_id gender expire_flag\n",
       "0        690      1           1\n",
       "1        704      1           1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final features left for patients_reduced dataset\n",
    "patients_reduced.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning dataset:  drgcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1941, 4)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drgcodes_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>avg_drg_mortality</th>\n",
       "      <th>avg_drg_severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6451</td>\n",
       "      <td>183196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4655</td>\n",
       "      <td>143283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23000</td>\n",
       "      <td>132906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject_id hadm_id  avg_drg_mortality  avg_drg_severity\n",
       "0       6451  183196                0.0               0.0\n",
       "1       4655  143283                0.0               0.0\n",
       "2      23000  132906                0.0               0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drgcodes_reduced.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dummy variables for drg_code\n",
    "# just_dummies = pd.get_dummies(drgcodes_reduced['drg_code'], prefix='drg_code')\n",
    "# drgcodes_reduced = pd.concat([drgcodes_reduced, just_dummies], axis=1)\n",
    "\n",
    "# # Create dummy variables for drg_code\n",
    "# just_dummies = pd.get_dummies(drgcodes_reduced['drg_type'], prefix='drg_type')\n",
    "# drgcodes_reduced = pd.concat([drgcodes_reduced, just_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transform object to numerical features\n",
    "# drgcodes_reduced['drg_mortality'] = pd.to_numeric(drgcodes_reduced.drg_mortality, errors='coerce').fillna(0, downcast='infer').astype('Int32')\n",
    "# drgcodes_reduced['drg_severity'] = pd.to_numeric(drgcodes_reduced.drg_severity, errors='coerce').fillna(0, downcast='infer').astype('Int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In order to have one record for each unique combination of subject_id and hadm_id, mean of the drg_mortality and drg_severity\n",
    "# # are calculated\n",
    "# drgcodes_reduced['avg_drg_mortality'] = drgcodes_reduced.groupby(['subject_id', 'hadm_id']).drg_mortality.transform('mean')\n",
    "# drgcodes_reduced['avg_drg_severity'] = drgcodes_reduced.groupby(['subject_id', 'hadm_id']).drg_severity.transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drgcodes_reduced['avg_drg_mortality'] = drgcodes_reduced.avg_drg_mortality.astype(float)\n",
    "# drgcodes_reduced['avg_drg_severity'] = drgcodes_reduced.avg_drg_severity.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates by comparing subject_id and hadm_id\n",
    "drgcodes_reduced = drgcodes_reduced.drop_duplicates(subset=['subject_id','hadm_id'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "drgcodes_reduced = drgcodes_reduced.drop(['row_id', 'description', 'drg_code', 'drg_type', 'drg_severity', 'drg_mortality' ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1941, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final size of the drgcodes_reduced\n",
    "drgcodes_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phenotypes_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main = pd.merge(admissions_reduced, phenotypes_reduced,\n",
    "                how ='right',\n",
    "                on = ['subject_id', 'hadm_id'])\n",
    "\n",
    "main = pd.merge(main, patients_reduced,\n",
    "                how ='left',\n",
    "                on = ['subject_id'])\n",
    "\n",
    "main = pd.merge(main, drgcodes_reduced,\n",
    "                how ='left',\n",
    "                on = ['subject_id', 'hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "main[\"expire_flag\"] = main.expire_flag.astype(float)\n",
    "main['gender'] = main.gender.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813, 103)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.shape # final dataset (813, 742)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missingness of the final merged datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id                                  0.000000\n",
       "hadm_id                                     0.000000\n",
       "hospital_expire_flag                        0.000000\n",
       "has_chartevents_data                        0.000000\n",
       "length_ed                                   0.303813\n",
       "length_admit                                0.000000\n",
       "admission_type_elective                     0.000000\n",
       "admission_type_emergency                    0.000000\n",
       "admission_type_urgent                       0.000000\n",
       "admission_loc_clinic referral/premature     0.000000\n",
       "admission_loc_emergency room admit          0.000000\n",
       "admission_loc_phys referral/normal deli     0.000000\n",
       "admission_loc_transfer from hosp/extram     0.000000\n",
       "admission_loc_transfer from other healt     0.000000\n",
       "admission_loc_transfer from skilled nur     0.000000\n",
       "discharge_loc_dead/expired                  0.000000\n",
       "discharge_loc_disc-tran cancer/chldrn h     0.000000\n",
       "discharge_loc_disch-tran to psych hosp      0.000000\n",
       "discharge_loc_home                          0.000000\n",
       "discharge_loc_home health care              0.000000\n",
       "discharge_loc_home with home iv providr     0.000000\n",
       "discharge_loc_hospice-home                  0.000000\n",
       "discharge_loc_hospice-medical facility      0.000000\n",
       "discharge_loc_icf                           0.000000\n",
       "discharge_loc_left against medical advi     0.000000\n",
       "discharge_loc_long term care hospital       0.000000\n",
       "discharge_loc_other facility                0.000000\n",
       "discharge_loc_rehab/distinct part hosp      0.000000\n",
       "discharge_loc_short term hospital           0.000000\n",
       "discharge_loc_snf                           0.000000\n",
       "insurance_government                        0.000000\n",
       "insurance_medicaid                          0.000000\n",
       "insurance_medicare                          0.000000\n",
       "insurance_private                           0.000000\n",
       "insurance_self pay                          0.000000\n",
       "religion_baptist                            0.000000\n",
       "religion_buddhist                           0.000000\n",
       "religion_catholic                           0.000000\n",
       "religion_christian scientist                0.000000\n",
       "religion_episcopalian                       0.000000\n",
       "religion_greek orthodox                     0.000000\n",
       "religion_hebrew                             0.000000\n",
       "religion_jehovah's witness                  0.000000\n",
       "religion_jewish                             0.000000\n",
       "religion_muslim                             0.000000\n",
       "religion_nan                                0.000000\n",
       "religion_not specified                      0.000000\n",
       "religion_other                              0.000000\n",
       "religion_protestant quaker                  0.000000\n",
       "religion_romanian east. orth                0.000000\n",
       "religion_unitarian-universalist             0.000000\n",
       "religion_unobtainable                       0.000000\n",
       "language_*chi                               0.000000\n",
       "language_*hun                               0.000000\n",
       "language_*man                               0.000000\n",
       "language_arab                               0.000000\n",
       "language_camb                               0.000000\n",
       "language_cant                               0.000000\n",
       "language_cape                               0.000000\n",
       "language_engl                               0.000000\n",
       "language_fren                               0.000000\n",
       "language_gree                               0.000000\n",
       "language_hait                               0.000000\n",
       "language_ital                               0.000000\n",
       "language_nan                                0.000000\n",
       "language_pers                               0.000000\n",
       "language_port                               0.000000\n",
       "language_ptun                               0.000000\n",
       "language_russ                               0.000000\n",
       "language_span                               0.000000\n",
       "language_urdu                               0.000000\n",
       "marital_status_divorced                     0.000000\n",
       "marital_status_life partner                 0.000000\n",
       "marital_status_married                      0.000000\n",
       "marital_status_nan                          0.000000\n",
       "marital_status_separated                    0.000000\n",
       "marital_status_single                       0.000000\n",
       "marital_status_unknown (default)            0.000000\n",
       "marital_status_widowed                      0.000000\n",
       "ethnicity_asian                             0.000000\n",
       "ethnicity_asian - asian indian              0.000000\n",
       "ethnicity_asian - chinese                   0.000000\n",
       "ethnicity_black/african                     0.000000\n",
       "ethnicity_black/african american            0.000000\n",
       "ethnicity_black/cape verdean                0.000000\n",
       "ethnicity_black/haitian                     0.000000\n",
       "ethnicity_hispanic or latino                0.000000\n",
       "ethnicity_hispanic/latino - guatemalan      0.000000\n",
       "ethnicity_hispanic/latino - puerto rican    0.000000\n",
       "ethnicity_other                             0.000000\n",
       "ethnicity_patient declined to answer        0.000000\n",
       "ethnicity_portuguese                        0.000000\n",
       "ethnicity_unable to obtain                  0.000000\n",
       "ethnicity_unknown/not specified             0.000000\n",
       "ethnicity_white                             0.000000\n",
       "ethnicity_white - brazilian                 0.000000\n",
       "ethnicity_white - eastern european          0.000000\n",
       "ethnicity_white - russian                   0.000000\n",
       "depression                                  0.000000\n",
       "gender                                      0.000000\n",
       "expire_flag                                 0.000000\n",
       "avg_drg_mortality                           0.000000\n",
       "avg_drg_severity                            0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.isnull().mean() # length_ed had ~30% missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "      <th>has_chartevents_data</th>\n",
       "      <th>length_ed</th>\n",
       "      <th>length_admit</th>\n",
       "      <th>admission_type_elective</th>\n",
       "      <th>admission_type_emergency</th>\n",
       "      <th>admission_type_urgent</th>\n",
       "      <th>admission_loc_clinic referral/premature</th>\n",
       "      <th>admission_loc_emergency room admit</th>\n",
       "      <th>admission_loc_phys referral/normal deli</th>\n",
       "      <th>admission_loc_transfer from hosp/extram</th>\n",
       "      <th>admission_loc_transfer from other healt</th>\n",
       "      <th>admission_loc_transfer from skilled nur</th>\n",
       "      <th>discharge_loc_dead/expired</th>\n",
       "      <th>discharge_loc_disc-tran cancer/chldrn h</th>\n",
       "      <th>discharge_loc_disch-tran to psych hosp</th>\n",
       "      <th>discharge_loc_home</th>\n",
       "      <th>discharge_loc_home health care</th>\n",
       "      <th>discharge_loc_home with home iv providr</th>\n",
       "      <th>discharge_loc_hospice-home</th>\n",
       "      <th>discharge_loc_hospice-medical facility</th>\n",
       "      <th>discharge_loc_icf</th>\n",
       "      <th>discharge_loc_left against medical advi</th>\n",
       "      <th>discharge_loc_long term care hospital</th>\n",
       "      <th>discharge_loc_other facility</th>\n",
       "      <th>discharge_loc_rehab/distinct part hosp</th>\n",
       "      <th>discharge_loc_short term hospital</th>\n",
       "      <th>discharge_loc_snf</th>\n",
       "      <th>insurance_government</th>\n",
       "      <th>insurance_medicaid</th>\n",
       "      <th>insurance_medicare</th>\n",
       "      <th>insurance_private</th>\n",
       "      <th>insurance_self pay</th>\n",
       "      <th>religion_baptist</th>\n",
       "      <th>religion_buddhist</th>\n",
       "      <th>religion_catholic</th>\n",
       "      <th>religion_christian scientist</th>\n",
       "      <th>religion_episcopalian</th>\n",
       "      <th>religion_greek orthodox</th>\n",
       "      <th>religion_hebrew</th>\n",
       "      <th>religion_jehovah's witness</th>\n",
       "      <th>religion_jewish</th>\n",
       "      <th>religion_muslim</th>\n",
       "      <th>religion_nan</th>\n",
       "      <th>religion_not specified</th>\n",
       "      <th>religion_other</th>\n",
       "      <th>religion_protestant quaker</th>\n",
       "      <th>religion_romanian east. orth</th>\n",
       "      <th>religion_unitarian-universalist</th>\n",
       "      <th>religion_unobtainable</th>\n",
       "      <th>language_*chi</th>\n",
       "      <th>language_*hun</th>\n",
       "      <th>language_*man</th>\n",
       "      <th>language_arab</th>\n",
       "      <th>language_camb</th>\n",
       "      <th>language_cant</th>\n",
       "      <th>language_cape</th>\n",
       "      <th>language_engl</th>\n",
       "      <th>language_fren</th>\n",
       "      <th>language_gree</th>\n",
       "      <th>language_hait</th>\n",
       "      <th>language_ital</th>\n",
       "      <th>language_nan</th>\n",
       "      <th>language_pers</th>\n",
       "      <th>language_port</th>\n",
       "      <th>language_ptun</th>\n",
       "      <th>language_russ</th>\n",
       "      <th>language_span</th>\n",
       "      <th>language_urdu</th>\n",
       "      <th>marital_status_divorced</th>\n",
       "      <th>marital_status_life partner</th>\n",
       "      <th>marital_status_married</th>\n",
       "      <th>marital_status_nan</th>\n",
       "      <th>marital_status_separated</th>\n",
       "      <th>marital_status_single</th>\n",
       "      <th>marital_status_unknown (default)</th>\n",
       "      <th>marital_status_widowed</th>\n",
       "      <th>ethnicity_asian</th>\n",
       "      <th>ethnicity_asian - asian indian</th>\n",
       "      <th>ethnicity_asian - chinese</th>\n",
       "      <th>ethnicity_black/african</th>\n",
       "      <th>ethnicity_black/african american</th>\n",
       "      <th>ethnicity_black/cape verdean</th>\n",
       "      <th>ethnicity_black/haitian</th>\n",
       "      <th>ethnicity_hispanic or latino</th>\n",
       "      <th>ethnicity_hispanic/latino - guatemalan</th>\n",
       "      <th>ethnicity_hispanic/latino - puerto rican</th>\n",
       "      <th>ethnicity_other</th>\n",
       "      <th>ethnicity_patient declined to answer</th>\n",
       "      <th>ethnicity_portuguese</th>\n",
       "      <th>ethnicity_unable to obtain</th>\n",
       "      <th>ethnicity_unknown/not specified</th>\n",
       "      <th>ethnicity_white</th>\n",
       "      <th>ethnicity_white - brazilian</th>\n",
       "      <th>ethnicity_white - eastern european</th>\n",
       "      <th>ethnicity_white - russian</th>\n",
       "      <th>depression</th>\n",
       "      <th>gender</th>\n",
       "      <th>expire_flag</th>\n",
       "      <th>avg_drg_mortality</th>\n",
       "      <th>avg_drg_severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3365</td>\n",
       "      <td>100103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27290</td>\n",
       "      <td>100137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5525</td>\n",
       "      <td>100473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41515</td>\n",
       "      <td>100485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2265</td>\n",
       "      <td>100548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject_id hadm_id  hospital_expire_flag  has_chartevents_data  length_ed  \\\n",
       "0       3365  100103                   0.0                   1.0        NaN   \n",
       "1      27290  100137                   0.0                   1.0        NaN   \n",
       "2       5525  100473                   0.0                   1.0        0.0   \n",
       "3      41515  100485                   0.0                   1.0        NaN   \n",
       "4       2265  100548                   0.0                   1.0        0.0   \n",
       "\n",
       "   length_admit  admission_type_elective  admission_type_emergency  \\\n",
       "0            10                        1                         0   \n",
       "1             7                        0                         1   \n",
       "2            16                        0                         1   \n",
       "3             4                        1                         0   \n",
       "4            13                        0                         1   \n",
       "\n",
       "   admission_type_urgent  admission_loc_clinic referral/premature  \\\n",
       "0                      0                                        0   \n",
       "1                      0                                        0   \n",
       "2                      0                                        0   \n",
       "3                      0                                        0   \n",
       "4                      0                                        0   \n",
       "\n",
       "   admission_loc_emergency room admit  \\\n",
       "0                                   0   \n",
       "1                                   0   \n",
       "2                                   1   \n",
       "3                                   0   \n",
       "4                                   1   \n",
       "\n",
       "   admission_loc_phys referral/normal deli  \\\n",
       "0                                        1   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        1   \n",
       "4                                        0   \n",
       "\n",
       "   admission_loc_transfer from hosp/extram  \\\n",
       "0                                        0   \n",
       "1                                        1   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "\n",
       "   admission_loc_transfer from other healt  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "\n",
       "   admission_loc_transfer from skilled nur  discharge_loc_dead/expired  \\\n",
       "0                                        0                           0   \n",
       "1                                        0                           0   \n",
       "2                                        0                           0   \n",
       "3                                        0                           0   \n",
       "4                                        0                           0   \n",
       "\n",
       "   discharge_loc_disc-tran cancer/chldrn h  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "\n",
       "   discharge_loc_disch-tran to psych hosp  discharge_loc_home  \\\n",
       "0                                       0                   0   \n",
       "1                                       0                   0   \n",
       "2                                       0                   0   \n",
       "3                                       0                   0   \n",
       "4                                       0                   0   \n",
       "\n",
       "   discharge_loc_home health care  discharge_loc_home with home iv providr  \\\n",
       "0                               1                                        0   \n",
       "1                               0                                        0   \n",
       "2                               0                                        0   \n",
       "3                               1                                        0   \n",
       "4                               0                                        0   \n",
       "\n",
       "   discharge_loc_hospice-home  discharge_loc_hospice-medical facility  \\\n",
       "0                           0                                       0   \n",
       "1                           0                                       0   \n",
       "2                           0                                       0   \n",
       "3                           0                                       0   \n",
       "4                           0                                       0   \n",
       "\n",
       "   discharge_loc_icf  discharge_loc_left against medical advi  \\\n",
       "0                  0                                        0   \n",
       "1                  0                                        0   \n",
       "2                  0                                        0   \n",
       "3                  0                                        0   \n",
       "4                  0                                        0   \n",
       "\n",
       "   discharge_loc_long term care hospital  discharge_loc_other facility  \\\n",
       "0                                      0                             0   \n",
       "1                                      0                             0   \n",
       "2                                      0                             0   \n",
       "3                                      0                             0   \n",
       "4                                      0                             0   \n",
       "\n",
       "   discharge_loc_rehab/distinct part hosp  discharge_loc_short term hospital  \\\n",
       "0                                       0                                  0   \n",
       "1                                       0                                  0   \n",
       "2                                       1                                  0   \n",
       "3                                       0                                  0   \n",
       "4                                       1                                  0   \n",
       "\n",
       "   discharge_loc_snf  insurance_government  insurance_medicaid  \\\n",
       "0                  0                     0                   0   \n",
       "1                  1                     0                   0   \n",
       "2                  0                     0                   0   \n",
       "3                  0                     0                   0   \n",
       "4                  0                     0                   0   \n",
       "\n",
       "   insurance_medicare  insurance_private  insurance_self pay  \\\n",
       "0                   1                  0                   0   \n",
       "1                   1                  0                   0   \n",
       "2                   1                  0                   0   \n",
       "3                   0                  1                   0   \n",
       "4                   0                  1                   0   \n",
       "\n",
       "   religion_baptist  religion_buddhist  religion_catholic  \\\n",
       "0                 0                  0                  1   \n",
       "1                 0                  0                  0   \n",
       "2                 0                  0                  0   \n",
       "3                 0                  0                  1   \n",
       "4                 0                  0                  1   \n",
       "\n",
       "   religion_christian scientist  religion_episcopalian  \\\n",
       "0                             0                      0   \n",
       "1                             0                      0   \n",
       "2                             0                      0   \n",
       "3                             0                      0   \n",
       "4                             0                      0   \n",
       "\n",
       "   religion_greek orthodox  religion_hebrew  religion_jehovah's witness  \\\n",
       "0                        0                0                           0   \n",
       "1                        0                0                           0   \n",
       "2                        0                0                           0   \n",
       "3                        0                0                           0   \n",
       "4                        0                0                           0   \n",
       "\n",
       "   religion_jewish  religion_muslim  religion_nan  religion_not specified  \\\n",
       "0                0                0             0                       0   \n",
       "1                1                0             0                       0   \n",
       "2                0                0             0                       0   \n",
       "3                0                0             0                       0   \n",
       "4                0                0             0                       0   \n",
       "\n",
       "   religion_other  religion_protestant quaker  religion_romanian east. orth  \\\n",
       "0               0                           0                             0   \n",
       "1               0                           0                             0   \n",
       "2               1                           0                             0   \n",
       "3               0                           0                             0   \n",
       "4               0                           0                             0   \n",
       "\n",
       "   religion_unitarian-universalist  religion_unobtainable  language_*chi  \\\n",
       "0                                0                      0              0   \n",
       "1                                0                      0              0   \n",
       "2                                0                      0              0   \n",
       "3                                0                      0              0   \n",
       "4                                0                      0              0   \n",
       "\n",
       "   language_*hun  language_*man  language_arab  language_camb  language_cant  \\\n",
       "0              0              0              0              0              0   \n",
       "1              0              0              0              0              0   \n",
       "2              0              0              0              0              0   \n",
       "3              0              0              0              0              0   \n",
       "4              0              0              0              0              0   \n",
       "\n",
       "   language_cape  language_engl  language_fren  language_gree  language_hait  \\\n",
       "0              0              0              0              0              0   \n",
       "1              0              1              0              0              0   \n",
       "2              0              1              0              0              0   \n",
       "3              0              1              0              0              0   \n",
       "4              0              0              0              0              0   \n",
       "\n",
       "   language_ital  language_nan  language_pers  language_port  language_ptun  \\\n",
       "0              0             1              0              0              0   \n",
       "1              0             0              0              0              0   \n",
       "2              0             0              0              0              0   \n",
       "3              0             0              0              0              0   \n",
       "4              0             1              0              0              0   \n",
       "\n",
       "   language_russ  language_span  language_urdu  marital_status_divorced  \\\n",
       "0              0              0              0                        0   \n",
       "1              0              0              0                        0   \n",
       "2              0              0              0                        0   \n",
       "3              0              0              0                        0   \n",
       "4              0              0              0                        0   \n",
       "\n",
       "   marital_status_life partner  marital_status_married  marital_status_nan  \\\n",
       "0                            0                       0                   0   \n",
       "1                            0                       1                   0   \n",
       "2                            0                       1                   0   \n",
       "3                            0                       1                   0   \n",
       "4                            0                       1                   0   \n",
       "\n",
       "   marital_status_separated  marital_status_single  \\\n",
       "0                         0                      0   \n",
       "1                         0                      0   \n",
       "2                         0                      0   \n",
       "3                         0                      0   \n",
       "4                         0                      0   \n",
       "\n",
       "   marital_status_unknown (default)  marital_status_widowed  ethnicity_asian  \\\n",
       "0                                 0                       1                0   \n",
       "1                                 0                       0                0   \n",
       "2                                 0                       0                0   \n",
       "3                                 0                       0                0   \n",
       "4                                 0                       0                0   \n",
       "\n",
       "   ethnicity_asian - asian indian  ethnicity_asian - chinese  \\\n",
       "0                               0                          0   \n",
       "1                               0                          0   \n",
       "2                               0                          0   \n",
       "3                               0                          0   \n",
       "4                               0                          0   \n",
       "\n",
       "   ethnicity_black/african  ethnicity_black/african american  \\\n",
       "0                        0                                 0   \n",
       "1                        0                                 0   \n",
       "2                        0                                 0   \n",
       "3                        0                                 0   \n",
       "4                        0                                 0   \n",
       "\n",
       "   ethnicity_black/cape verdean  ethnicity_black/haitian  \\\n",
       "0                             0                        0   \n",
       "1                             0                        0   \n",
       "2                             0                        0   \n",
       "3                             0                        0   \n",
       "4                             0                        0   \n",
       "\n",
       "   ethnicity_hispanic or latino  ethnicity_hispanic/latino - guatemalan  \\\n",
       "0                             0                                       0   \n",
       "1                             0                                       0   \n",
       "2                             0                                       0   \n",
       "3                             0                                       0   \n",
       "4                             0                                       0   \n",
       "\n",
       "   ethnicity_hispanic/latino - puerto rican  ethnicity_other  \\\n",
       "0                                         0                0   \n",
       "1                                         0                0   \n",
       "2                                         0                0   \n",
       "3                                         0                0   \n",
       "4                                         0                0   \n",
       "\n",
       "   ethnicity_patient declined to answer  ethnicity_portuguese  \\\n",
       "0                                     0                     0   \n",
       "1                                     0                     0   \n",
       "2                                     0                     0   \n",
       "3                                     0                     0   \n",
       "4                                     0                     0   \n",
       "\n",
       "   ethnicity_unable to obtain  ethnicity_unknown/not specified  \\\n",
       "0                           0                                0   \n",
       "1                           0                                0   \n",
       "2                           0                                0   \n",
       "3                           0                                0   \n",
       "4                           0                                0   \n",
       "\n",
       "   ethnicity_white  ethnicity_white - brazilian  \\\n",
       "0                1                            0   \n",
       "1                1                            0   \n",
       "2                1                            0   \n",
       "3                1                            0   \n",
       "4                1                            0   \n",
       "\n",
       "   ethnicity_white - eastern european  ethnicity_white - russian depression  \\\n",
       "0                                   0                          0          0   \n",
       "1                                   0                          0          0   \n",
       "2                                   0                          0          0   \n",
       "3                                   0                          0          1   \n",
       "4                                   0                          0          0   \n",
       "\n",
       "   gender  expire_flag  avg_drg_mortality  avg_drg_severity  \n",
       "0       0          0.0           0.000000          0.000000  \n",
       "1       1          1.0           2.000000          2.000000  \n",
       "2       1          1.0           2.666667          2.666667  \n",
       "3       0          0.0           0.666667          1.333333  \n",
       "4       1          1.0           0.000000          0.000000  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "main['length_ed'] = main['length_ed'].fillna(0) # not entirely sure about this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructuring -- Move the outcome variable to be the last column in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(main.columns.values)\n",
    "cols.pop(cols.index('depression'))\n",
    "main = main[cols+['depression']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "main[\"hospital_expire_flag\"] = main.hospital_expire_flag.astype(float)\n",
    "main[\"has_chartevents_data\"] = main.has_chartevents_data.astype(float)\n",
    "main[\"expire_flag\"] = main.expire_flag.astype(float)\n",
    "main['gender'] = main.gender.astype(int)\n",
    "main['avg_drg_mortality'] = main.avg_drg_mortality.astype(float)\n",
    "main['avg_drg_severity'] = main.avg_drg_severity.astype(float)\n",
    "main['depression'] = main.depression.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set X as features and y as the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = main.drop(['subject_id', 'hadm_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High correlation filter\n",
    "\n",
    "This dimensionality reduction algorithm tries to discard inputs that are very similar to others. In simple words, if your opinion is same as your boss, one of you is not required. If the value of two input parameters is always the same, it means they represent the same entity. Then we do not need two parameters there. Just one should be enough.\n",
    "\n",
    "In technical words, if there is a very high correlation between two input variables, we can safely drop one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Absolute Correlations\n",
      "language_hait             ethnicity_black/haitian                    1.000000\n",
      "hospital_expire_flag      discharge_loc_dead/expired                 1.000000\n",
      "avg_drg_mortality         avg_drg_severity                           0.955766\n",
      "religion_buddhist         language_camb                              0.865491\n",
      "admission_type_elective   admission_type_emergency                   0.851231\n",
      "language_engl             language_nan                               0.824003\n",
      "language_port             ethnicity_white - brazilian                0.815993\n",
      "admission_type_emergency  admission_loc_phys referral/normal deli    0.811482\n",
      "admission_type_elective   admission_loc_phys referral/normal deli    0.810679\n",
      "language_cape             ethnicity_black/cape verdean               0.706671\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]\n",
    "\n",
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(main, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing highly correlated features\n",
    "main = main.drop(['language_hait', 'discharge_loc_dead/expired'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_drg_mortality         avg_drg_severity                           0.955766\n",
      "religion_buddhist         language_camb                              0.865491\n",
      "admission_type_elective   admission_type_emergency                   0.851231\n",
      "language_engl             language_nan                               0.824003\n",
      "language_port             ethnicity_white - brazilian                0.815993\n",
      "admission_type_emergency  admission_loc_phys referral/normal deli    0.811482\n",
      "admission_type_elective   admission_loc_phys referral/normal deli    0.810679\n",
      "language_cape             ethnicity_black/cape verdean               0.706671\n",
      "religion_muslim           language_urdu                              0.705796\n",
      "insurance_medicare        insurance_private                          0.697338\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(get_top_abs_correlations(main, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = main['depression']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813, 98)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = main.drop(['depression'], axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXgA6CzlqbCl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling - Standardization vs. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is essential for machine learning algorithms that calculate distances between data. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\n",
    "\n",
    "* Normalization is recommended when you have a normally distributed observations.\n",
    "* Standardization works all the time. (recommended)\n",
    "* We need to perform Feature Scaling when we are dealing with Gradient Descent Based algorithms (Linear and Logistic Regression, Neural Network) and Distance-based algorithms (KNN, K-means, SVM) as these are very sensitive to the range of the data points.\n",
    "\n",
    "* It is a good practice to fit the scaler on the training data and then use it to transform the testing data. This would avoid any data leakage during the model testing process. Also, the scaling of target values is generally not required.\n",
    "* Only apply standardization to numerical columns and not the other One-Hot Encoded features. Standardizing the One-Hot encoded features would mean assigning a distribution to categorical features. You donâ€™t want to do that! While it is fine to apply normalization to all kinds of columns including One-Hot Encorded features because One-Hot encoded features are already in the range between 0 to 1. So, normalization would not affect their value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# fit scaler on training data\n",
    "norm = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# transform training data\n",
    "X_train_norm = norm.transform(X_train)\n",
    "\n",
    "# transofrm teting data\n",
    "X_test_norm = norm.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization with sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# copy of datasets\n",
    "X_train_stand = X_train.copy()\n",
    "X_test_stand = X_test.copy()\n",
    "\n",
    "# numerical features\n",
    "num_cols = ['length_admit', 'avg_drg_mortality', 'avg_drg_severity']\n",
    "\n",
    "# apply standardization on numerical features\n",
    "for i in num_cols:\n",
    "    scale = StandardScaler().fit(X_train_stand[[i]])\n",
    "    X_train_stand[i] = scale.transform(X_train_stand[[i]])\n",
    "    X_test_stand[i] = scale.transform(X_test_stand[[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction is a data preparation technique performed on data prior to modeling. It might be performed after data cleaning and data scaling and before training a predictive model.\n",
    "\n",
    "As such, any dimensionality reduction performed on training data must also be performed on new data, such as a test dataset, validation dataset, and data when making a prediction with the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "Standardization: All the variables should be on the same scale before applying PCA, otherwise, a feature with large values will dominate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 10)\n",
    "X_train_norm_pca = pca.fit_transform(X_train_norm)\n",
    "X_test_norm_pca = pca.transform(X_test_norm)\n",
    "X_train_stand_pca = pca.fit_transform(X_train_stand)\n",
    "X_test_stand_pca = pca.transform(X_test_stand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[132   3]\n",
      " [ 26   2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8220858895705522"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test_norm_pca)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[133   2]\n",
      " [ 27   1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8220858895705522"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train_stand_pca, y_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test_stand_pca)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[120  15]\n",
      " [ 18  10]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7975460122699386"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) # for Euclidean distance\n",
    "classifier.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "# predicting the test set results\n",
    "y_pred = classifier.predict(X_test_norm_pca)\n",
    "\n",
    "# making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124  11]\n",
      " [ 21   7]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.803680981595092"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) # for Euclidean distance\n",
    "classifier.fit(X_train_stand_pca, y_train)\n",
    "\n",
    "# predicting the test set results\n",
    "y_pred = classifier.predict(X_test_stand_pca)\n",
    "\n",
    "# making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption: observations are linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[135   0]\n",
      " [ 28   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8282208588957055"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[135   0]\n",
      " [ 28   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8282208588957055"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train_stand_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maaping to a higher dimensional space can be highly compute-intensive.\n",
    "\n",
    "Types of Kernal Functions:\n",
    "* Gaussian RBF Kernel\n",
    "* Sigmoid Kernel\n",
    "* Polynomial Kernel\n",
    "* and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[135   0]\n",
      " [ 28   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8282208588957055"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[135   0]\n",
      " [ 28   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8282208588957055"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train_stand_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages: This algorithm requires a small amount of training data to estimate the necessary parameters. Naive Bayes classifiers are extremely fast compared to more sophisticated methods.\n",
    "\n",
    "Disadvantages: Naive Bayes is is known to be a bad estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[130   5]\n",
      " [ 26   2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8098159509202454"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[131   4]\n",
      " [ 26   2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8159509202453987"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train_stand_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages: Decision Tree is simple to understand and visualise, requires little data preparation, and can handle both numerical and categorical data.\n",
    "\n",
    "Disadvantages: Decision tree can create complex trees that do not generalise well, and decision trees can be unstable because small variations in the data might result in a completely different tree being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[108  27]\n",
      " [ 18  10]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7239263803680982"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[95 40]\n",
      " [14 14]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6687116564417178"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train_stand_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Learning : using different machine algorithms \n",
    "\n",
    "The algorithm does not work well for datasets having a lot of outliers, something which needs addressing prior to the model building.\n",
    "\n",
    "Advantages: Reduction in over-fitting and random forest classifier is more accurate than decision trees in most cases.\n",
    "\n",
    "Disadvantages: Slow real time prediction, difficult to implement, and complex algorithm.\n",
    "\n",
    "Build on top of Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[126   9]\n",
      " [ 23   5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.803680981595092"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[127   8]\n",
      " [ 25   3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7975460122699386"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train_stand_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_pca)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# fit scaler on training data\n",
    "norm = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# transform training data\n",
    "X_train_norm = norm.transform(X_train)\n",
    "\n",
    "# transofrm teting data\n",
    "X_test_norm = norm.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization with sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# copy of datasets\n",
    "X_train_stand = X_train.copy()\n",
    "X_test_stand = X_test.copy()\n",
    "\n",
    "# numerical features\n",
    "num_cols = ['length_admit', 'avg_drg_mortality', 'avg_drg_severity']\n",
    "\n",
    "# apply standardization on numerical features\n",
    "for i in num_cols:\n",
    "    scale = StandardScaler().fit(X_train_stand[[i]])\n",
    "    X_train_stand[i] = scale.transform(X_train_stand[[i]])\n",
    "    X_test_stand[i] = scale.transform(X_test_stand[[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components = 1)\n",
    "X_train_norm_lda = lda.fit_transform(X_train_norm, y_train)\n",
    "X_test_norm_lda = lda.transform(X_test_norm)\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components = 1)\n",
    "X_train_stand_lda = lda.fit_transform(X_train_stand, y_train)\n",
    "X_test_stand_lda = lda.transform(X_test_stand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[119  16]\n",
      " [ 23   5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7607361963190185"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train_norm_lda, y_train)\n",
    "\n",
    "# predicting the test set results\n",
    "y_pred = classifier.predict(X_test_norm_lda)\n",
    "\n",
    "# Making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[119  16]\n",
      " [ 23   5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7607361963190185"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train_stand_lda, y_train)\n",
    "\n",
    "# predicting the test set results\n",
    "y_pred = classifier.predict(X_test_stand_lda)\n",
    "\n",
    "# Making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbor (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbor (KNN) algorithm predicts based on the specified number (k) of the nearest neighboring data points. Here, the pre-processing of the data is significant as it impacts the distance measurements directly. Unlike others, the model does not have a mathematical formula, neither any descriptive ability.\n",
    "\n",
    "It is a simple, fairly accurate model preferable mostly for smaller datasets, owing to huge computations involved on the continuous predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Choose the number of K of neighbors\n",
    "\n",
    "Step 2: Take the K nearest neighbors of the new data point, according to the Euclidean distance\n",
    "\n",
    "Step 3: Among these K neighbors, count the number of data points in each category\n",
    "\n",
    "Step 4: Assign the new data point ot the category where you counted the most neighbors\n",
    "\n",
    "Then, the model will be ready. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[119  16]\n",
      " [ 22   6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7668711656441718"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) # for Euclidean distance\n",
    "classifier.fit(X_train_norm_lda, y_train)\n",
    "\n",
    "# predicting the test set results\n",
    "y_pred = classifier.predict(X_test_norm_lda)\n",
    "\n",
    "# making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[119  16]\n",
      " [ 22   6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7668711656441718"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) # for Euclidean distance\n",
    "classifier.fit(X_train_stand_lda, y_train)\n",
    "\n",
    "# predicting the test set results\n",
    "y_pred = classifier.predict(X_test_stand_lda)\n",
    "\n",
    "# making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption: observations are linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[127   8]\n",
      " [ 25   3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7975460122699386"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train_norm_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[127   8]\n",
      " [ 25   3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7975460122699386"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train_stand_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maaping to a higher dimensional space can be highly compute-intensive.\n",
    "\n",
    "Types of Kernal Functions:\n",
    "* Gaussian RBF Kernel\n",
    "* Sigmoid Kernel\n",
    "* Polynomial Kernel\n",
    "* and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[127   8]\n",
      " [ 25   3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7975460122699386"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train_norm_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[127   8]\n",
      " [ 25   3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7975460122699386"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train_stand_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages: This algorithm requires a small amount of training data to estimate the necessary parameters. Naive Bayes classifiers are extremely fast compared to more sophisticated methods.\n",
    "\n",
    "Disadvantages: Naive Bayes is is known to be a bad estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[119  16]\n",
      " [ 23   5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7607361963190185"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train_norm_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[119  16]\n",
      " [ 23   5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7607361963190185"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train_stand_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages: Decision Tree is simple to understand and visualise, requires little data preparation, and can handle both numerical and categorical data.\n",
    "\n",
    "Disadvantages: Decision tree can create complex trees that do not generalise well, and decision trees can be unstable because small variations in the data might result in a completely different tree being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[93 42]\n",
      " [17 11]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6380368098159509"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train_norm_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[93 42]\n",
      " [17 11]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6380368098159509"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train_stand_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Learning : using different machine algorithms \n",
    "\n",
    "The algorithm does not work well for datasets having a lot of outliers, something which needs addressing prior to the model building.\n",
    "\n",
    "Advantages: Reduction in over-fitting and random forest classifier is more accurate than decision trees in most cases.\n",
    "\n",
    "Disadvantages: Slow real time prediction, difficult to implement, and complex algorithm.\n",
    "\n",
    "Build on top of Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[97 38]\n",
      " [19  9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6503067484662577"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train_norm_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_norm_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[97 38]\n",
      " [19  9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6503067484662577"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train_stand_lda, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_stand_lda)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "1. Try GridSearch with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing_tools.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
